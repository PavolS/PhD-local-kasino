 \documentclass[1p]{elsarticle}
\usepackage{amssymb,latexsym}
\usepackage{enumerate}


%%%% Put my macros here:
%%%%%%%%%%%%%%%%%%%%%%%%%%%   usepackage   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%   usepackage   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%   usepackage   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{graphicx, color}
\usepackage{amsmath,amsthm,amssymb,amscd}
\usepackage[all]{xy}

\newcommand{\usftext}[1]{\textsf{\upshape #1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%   Kohlenbach   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%   Kohlenbach   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%   Kohlenbach   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\parindent0pt


\newcommand{\ba}{\begin{array}} \newcommand{\ea}{\end{array}}
\newfont{\bsl}{cmbxsl10 scaled 1095}




\newcommand{\ra}{\rightarrow}
\newcommand{\Ra}{\Rightarrow}
\newcommand{\Lra}{\Leftrightarrow}
\newcommand{\se}{\subseteq}
\newcommand{\si}{\wedge}
\newcommand{\sau}{\vee}
\newcommand{\ol}{\overline}
\newcommand{\nin}{\in\!\!\!\!\!/}
%\renewcommand{\topmargin}{-1cm}
\newfont{\deu}{eufm10 scaled 1000}
\newcommand{\scripta}{\mbox{{\deu A}}}
%\newcommand{\nvdash}{\mathop{\vdash\!\!\!\!\!/}\nolimits}
\newcommand{\nmodels}{\mathop{\models\!\!\!\!\!\!/}\nolimits}
\newcommand{\res}{|\!\raisebox{1mm}{$\scriptscriptstyle\setminus$}}
\newcommand{\remin}{\mathop{-\!\!\!\!\!\hspace*{1mm}\raisebox{0.5mm}{$
\cdot$}}\nolimits}
\newcommand{\aquant}{\forall}
\newcommand{\equant}{\exists}

\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}

%\topmargin 0 pt
%       \textwidth 434pt 
%       \oddsidemargin 24pt
%       \evensidemargin 10pt
%       \marginparwidth 42pt

%\openup1\jot
%%\sloppy
%\renewcommand{\textheight}{220mm}
%\begin{document}




%%%%%%%%%%%%%%%%%%%%%%%%%%%   Layout   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%   Layout   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%   Layout   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \setlength{\parskip}{1.5ex plus .3ex minus .2ex}        % Distance between paragraphs
%                                                         % +- indicates tolerance for TeX
% 

%%margins      
%\newlength{\topmarginmod}
%\newlength{\bottommarginmod}
%\topmarginmod=0.5in
%\bottommarginmod=0.4in
%\addtolength{\topmargin}{-\topmarginmod}
%\addtolength{\textheight}{\topmarginmod}
%\addtolength{\textheight}{\bottommarginmod}
%% so it gets all on a page
%\textwidth=13.4cm
%\oddsidemargin=0.9cm
%%should be pagewidth-1in-\oddsidemargin-\textwidth-1in~=21cm-2.8cm-1cm-13cm-2.8cm=1.4cm
%\evensidemargin=1.92cm

\newcommand{\todo}[1]{{\it #1}
  \marginpar{\center\texttt{ToDo}}}    % Issues to clarify

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% get a blank page %%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Quote  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\theQuote}[1]{
\begin{center}
\begin{minipage}{0.7\textwidth} 
{\em #1} 
\end{minipage}
\end{center}
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%   Remarks   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%   Remarks   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%   Remarks   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\rem}{{\tt ?}\marginpar{\Large \tt\centering ?}}
\newcommand{\Rem}[1]{{\tt ?}\marginpar{\raggedright #1}}
\newcommand{\OK}{\marginpar{\begin{center}\unitlength1.5em
        \begin{picture}(1,1) \put(0.5,0.5){\circle{1}}
        \put(0.5,0.4){\makebox(0,0){$\smile$}}
        \put(0.4,0.7){\makebox(0,0){$\cdot$}}
        \put(0.6,0.7){\makebox(0,0){$\cdot$}}
        \end{picture}\end{center}}}
\newcommand{\nOK}{\marginpar{\begin{center}\unitlength1.5em
        \begin{picture}(1,1) \put(0.5,0.5){\circle{1}}
        \put(0.5,0.5){\makebox(0,0){$\ddot{\frown}$}}
        \end{picture}\end{center}}}

%\newenvironment{rmk}{\paragraph{Remark}}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%   equations   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\be}[1][{e:\arabic{equation}}] { \begin{equation}\label{#1} }
\newcommand{\ee} { \end{equation} }



%%%%%%%%%%%%%%%%%%%%%%%%%%%%   General Maths   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%   General Maths   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%   General Maths   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%stuff - mainly KL
\DeclareMathOperator{\lh}{lh}  %length of encoding of a finite sequence
\DeclareMathOperator{\TMaj}{Maj}
\DeclareMathOperator{\TAN}{W}
\DeclareMathOperator{\Id}{id}
\DeclareMathOperator{\Fluc}{Fluc}
%\DeclareMathOperator{\P}{P}  %secured

%%commands
\renewcommand{\emptyset}{\varnothing}

\newcommand{\ORi}[1]{\ensuremath{\bigwedge^{#1}_{i=1}}}


\newcommand{\RR}{\ensuremath{\mathbb{R}}}
\newcommand{\NN}{\ensuremath{\mathbb{N}}}
\newcommand{\QQ}{\ensuremath{\mathbb{Q}}}
\newcommand{\II}{\ensuremath{\mathbb{I}}}

\newcommand{\zero}{\ensuremath{\mathbf0}}
\newcommand{\one}{\ensuremath{\mathbf1}}
\newcommand{\two}{\ensuremath{\mathbf2}}

\newcommand{\xor}{\ensuremath{\dot\vee}}

%%%%%%%%%%%%%%%%%%%%%%%%%   Proof Theory   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%   Proof Theory   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%   Proof Theory   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% stuff
%\input prooftree

\DeclareMathOperator{\maj}{maj} %``majorizes
\DeclareMathOperator{\smaj}{s-maj} %``strongly majorizes
\DeclareMathOperator{\K}{K} %K_A from Howard's WKL ND-int
\DeclareMathOperator{\I}{I} %the ``in Interval predicate from BW



%% Types
\newcommand{\Tp}{\ensuremath{\emph{\protect\textbf{T}}}} %set of finite types T
\newcommand{\PT}{\ensuremath{\emph{\protect\textbf{P}}}} %set of pure types P
\newcommand{\tp}[1]{\ensuremath{^\mathbf{#1}}}

%% Models
\newcommand {\SetO}  { \ensuremath{\mathcal{S} } }
\newcommand {\Som}  { \ensuremath{\SetO^\omega} }
\newcommand {\Set}  { \Som }
\newcommand {\ContO}  { \ensuremath{\mathcal{C} } }
\newcommand {\Cont}  { \ensuremath{\ContO^\omega} }
\newcommand {\MajO}  { \ensuremath{\mathcal{M} } }
\newcommand {\Maj}  { \ensuremath{\MajO^\omega} }

%% Special sets
\newcommand{\universal}{\ensuremath{\emph{\protect\textbf{U}}}} %set of universal axioms

%% Systems
\newcommand{\Ax}{\ensuremath{\mathcal{A}^\omega}} %GnA iaft
\newcommand{\AHilb}{\ensuremath{\mathcal{A}^\omega[X,\langle\cdot,\cdot\rangle]}} %A Hilbert space
\newcommand{\AHilbS}{\ensuremath{\mathcal{A}^\omega[X,\langle\cdot,\cdot\rangle,S]}} %A Hilbert sp. + S
\newcommand{\GA}{\ensuremath{\usftext{G}_n\usftext{A}^\omega}} %GnA iaft
\newcommand{\weha}{\ensuremath{{\usftext{WE-HA}}^{\omega}}} % WE - HA iaft
\newcommand{\wepa}{\ensuremath{{\usftext{WE-PA}}^{\omega}}} % WE - PA iaft
\newcommand{\HA}{\ensuremath{{\usftext{HA}}}} % HA 
\newcommand{\PA}{\ensuremath{\usftext{PA}}} % PA 
\newcommand{\ha}{\ensuremath{{\usftext{HA}}^\omega}} % HA iaft
\newcommand{\pa}{\ensuremath{{\usftext{PA}}^\omega}} % PA iaft
\newcommand{\epa}{\ensuremath{{\usftext{E-PA}}^\omega}} % E - PA iaft
\newcommand{\eha}{\ensuremath{{\usftext{E-HA}}^\omega}} % E - HA iaft
\newcommand{\hrrepa}{\ensuremath{\widehat{\usftext{E-PA}}^\omega\kleene}} %hrr E - PA iaft
\newcommand{\hrreha}{\ensuremath{\widehat{\usftext{E-HA}}^\omega\kleene}} %hrr E - HA iaft
\newcommand{\rreha}{\ensuremath{\usftext{E-HA}^\omega\kleene}} %rr E - HA iaft
\newcommand{\rrweha}{\ensuremath{\usftext{WE-HA}^\omega\kleene}} %rr WE - HA iaft
\newcommand{\kleene}{\ensuremath{\!\!\!\restriction}}   % upper arrow
\newcommand{\hrrwepa}{\ensuremath{\widehat{\usftext{WE-PA}}^\omega\kleene}} %hrr WE - PA iaft
\newcommand{\hrrweha}{\ensuremath{\widehat{\usftext{WE-HA}}^\omega\kleene}} %hrr WE - HA iaft

\newcommand{\HAS}{\ensuremath{\usftext{HAS}}} %second order logic
\newcommand{\HAH}{\ensuremath{\usftext{HAH}}} %higher -/-
\newcommand{\ACA}{\ensuremath{\usftext{ACA}}} %
\newcommand{\RCA}{\ensuremath{\usftext{RCA}}} %

%% Principles
\newcommand{\IA}{\ensuremath{\usftext{IA}}} %induction schema
\newcommand{\IP}{\ensuremath{\usftext{IP}}} %induction principle
\newcommand{\IR}{\ensuremath{\usftext{IR}}} %induction rule
\newcommand{\BR}{\ensuremath{\usftext{BR}}} %induction rule


\newcommand{\IPP}{\ensuremath{\usftext{IPP}}}
\newcommand{\PCM}{\ensuremath{\usftext{PCM}}}
\newcommand{\LEM}{\ensuremath{\Sigma^0_1\usftext{-LEM}}}
\newcommand{\lLEM}{\ensuremath{\usftext{LEM}}}
\newcommand{\CP}{\ensuremath{\usftext{CP}}}
\newcommand{\BW}{\ensuremath{\usftext{BW}}}
\renewcommand{\AA}{\ensuremath{\usftext{AA}}}
\newcommand{\Limsup}{\ensuremath{\usftext{Limsup}}}
\newcommand{\DNS}{\ensuremath{\usftext{DNS}}}
\newcommand{\DNE}{\ensuremath{\usftext{DNE}}}
\newcommand{\CA}{\ensuremath{\usftext{CA}}}
\newcommand{\QF}{\ensuremath{\usftext{QF}}}
\newcommand{\QFm}{\ensuremath{\usftext{QF-}}}
\newcommand{\CAhut}{\ensuremath{\widehat{\CA}}}

\newcommand{\AC}{\ensuremath{\usftext{AC}}} 
\newcommand{\ER}{\ensuremath{\usftext{ER}}} 

\newcommand{\WKL}{\ensuremath{\usftext{WKL}}}
\newcommand{\FAN}{\ensuremath{\usftext{FAN}}}

%% General abreviations
\newcommand{\PiL}{\ensuremath{\Pi^0_1}} 
\newcommand{\PiLm}{\ensuremath{\Pi^0_1\usftext{-}}} 
\newcommand{\SiL}{\ensuremath{\Sigma^0_1}} 
\newcommand{\SiLm}{\ensuremath{\Sigma^0_1\usftext{-}}} 
\newcommand{\m}{\ensuremath{\usftext{-}}}


%% for WKL

\newcommand{\BTree}{\ensuremath{\usftext{BinTree}}}
\newcommand{\BFunc}{\ensuremath{\usftext{BinFunc}}}
\newcommand{\UnBounded}{\ensuremath{\usftext{Unbounded}}}
%\newcommand{\Bounded}{\ensuremath{\usftext{Bounded}}}
\newcommand{\Sec}{\ensuremath{\usftext{Sec}}} % Boundedly secured
\newcommand{\BSec}{\ensuremath{\usftext{BarSec}}} % Boundedly secured at bar k
\newcommand{\BSecA}{\ensuremath{\usftext{BarSec}_A}} % Boundedly secured at bar K_A

\newcommand{\B}{\ensuremath{\usftext{B}}} %bar recursor
\newcommand{\rB}{\ensuremath{\usftext{B'}}} %restricted bar recursor
\newcommand{\R}{\ensuremath{\usftext{R}}} %recursor
\newcommand{\bPhi}{                       %special bar recursor
 \raisebox{-1.0pt} {
   \ensuremath{\usftext{\Large {\!$\Phi$\!}}}
 }
}

\newcommand{\T}{\ensuremath{\mathcal{T}}} %G???els T
\newcommand{\M}{\ensuremath{\usftext{M}^\omega}} %Markov principle iaft
\renewcommand{\H}{\ensuremath{\usftext{H}}} %Funny Howards argument

\newcommand{\proves}{\vdash}  %proves |-
\newcommand{\forces}{\Vdash}  %||-
\renewcommand{\models}{\vDash}  %|=



\newcommand{\tup}{\underline} %tuple
\newcommand{\atup}{\ensuremath{\,\underline}} %tuple as a parameter

\newcommand{\Tif}{\text{if}\ }
\newcommand{\Telse}{\text{otherwise}}

%%%%%%%%%%%%%%%%%%%%%%%%%   Theorems   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%   Theorems   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%   Theorems   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
%\newtheorem*{lemma*}{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{con}[thm]{Conjecture}

\theoremstyle{definition}
\newtheorem{dfn}[thm]{Definition}
\newtheorem{rmk}[thm]{Remark}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{eg}{Example}

%\newenvironment{lemma*}[2][]{\noindent{\bf Recall Lemma~\ref{#2}} ({#1}).}{\\}
\newenvironment{lemma*}[2][]{\noindent{\bf Recall Lemma~\ref{#2}} ({#1}). \begin{it}}{\end{it}\\}



%  from Klaus:
 \renewenvironment{proof}[1][]{\noindent{\bf Proof{#1}. }}{\nopagebreak[4]{\hspace*{\fill}
%   \rule{1.2ex}{1.2ex} % Full big box
  $\Box$              % Empty box
 }{\vspace{2ex}}}

%%%%%%%%%%%%%%%% get it stylish - shortcuts %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%% get it stylish - shortcuts %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%% get it stylish - shortcuts %%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\phi}{\varphi}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\lb}{\linebreak[0]}
\newcommand{\pb}{\pagebreak[0]}
\newcommand{\nbd}{\nobreakdash-}

\newcommand{\lOrd}[1]{\text{$\!\!$
\begin{smaller}<\end{smaller}\nolinebreak[4] $\!#1$\hspace{-3pt}
}}

\newcommand{\lOrdm}[1]{\text{
\lOrd{#1}\nbd 
%\hspace{-4pt}
}}

%\makeatletter
\def\Ddots{\mathinner{\mkern1mu\raise\p@
\vbox{\kern7\p@\hbox{.}}\mkern2mu
\raise4\p@\hbox{.}\mkern2mu\raise7\p@\hbox{.}\mkern1mu}}
%\makeatother

\newcommand{\embeded}{\hookrightarrow}



%% A numbered theorem with a fancy name:

\newtheorem{mainthm}[thm]{Main Theorem}

%% Numbered objects of "non-theorem" style (text roman):

\theoremstyle{definition}
\newtheorem{defin}[thm]{Definition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{exa}[thm]{Example}

\begin{document}


%%%%%%%%%%%


\title{Fluctuations, effective learnability and metastability in analysis}
\author{Ulrich Kohlenbach, Pavol Safarik}
\ead{kohlenbach@mathematik.tu-darmstadt.de, pavol.safarik@gmail.com}
\address{Department of Mathematics, Technische Universit\"at Darmstadt, Schlossgartenstra{\ss}e 7, 64289 Darmstadt, Germany}

\date{}

\begin{abstract}
\end{abstract}

\begin{keyword}
Fluctuations, effective learnability, metastability, proof mining, uniform 
bounds, functionals of finite type, nonlinear ergodic theory, hard analysis.\\
\MSC[2010]{03F10, 03F60, 47H25, 37A30}.
\end{keyword}


\maketitle


\section{Introduction}

\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}[definition]{Proposition}
%\newtheorem{remark}[definition]{Remark}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{corollary}[definition]{Corollary}
%\newtheorem{lemma}[definition]{Lemma}
\newtheorem{exercise}[definition]{Exercise}
\newtheorem{clm}[definition]{Claim}
%\newtheorem{prop}[definition]{Proposition}
\newtheorem{example}[definition]{Example}
\newtheorem{notation}[definition]{Notation}
\newtheorem{application}[definition]{Application} 


In this paper we investigate different levels of effective quantitative 
information on theorems stating the Cauchy property of some sequence 
$(x_n)$ in a metric space $(X,d)$
\[ (1) \ \forall k\in\NN\,\exists n\in\NN\,\forall m,\tilde{m}\ge n
\ \big( d(x_m,
x_{\tilde{m}})\le 2^{-k}\big) \] 
and also more general $\Pi^0_3$-theorems  
\[ (2) \ \varphi\equiv 
\forall k\in\NN\,\exists n\in\NN\,
\forall m\in\NN\,\varphi_0(k,n,m),\] where $\varphi_0$ is quantifier-free. 
Since we refer to real numbers as fast 
converging Cauchy sequences of rational numbers we have $\le_{\RR}\,\in\Pi^0_1$ 
so that $(1)$ has the form $(2).$
\\[2mm] 
Cauchy statements $(1)$ are special forms of finiteness statements expressing 
that there are only finitely many $2^{-k}$-fluctuations $(i_l,j_l)$ 
with 
\[ (3) \ j_l>i_l\wedge d(x_{i_l},x_{j_l})>2^{-k}. \] 
As with general finiteness statements one can ask for a bound on the height 
of $2^{-k}$-fluctuations, i.e. an $\rho(k)$ above which no such fluctuation 
occurs (i.e. $\rho$ is a rate of convergence) 
or for a weaker bound $F(k)$ on the number $l$ 
of such fluctuations $(i_0,j_0),\ldots,(i_l,j_l)$ with $i_{n+1}\le j_{n}$ 
for $n<l.$ 
As to be expected from standard recursion theoretic facts about finiteness 
statements (see \cite{Luckhardt(89)}), even primitive recursive Cauchy sequences $(x_n)$ in $\RR$ 
in general will not admit a computable (in $k$) bound $F$ on the fluctuations 
and even in cases where they do there will in general be no computable 
rate of convergence $\rho.$\\[2mm] 
A yet weaker information than a bound $F$ on the number of fluctuations is 
a bound on the Kreisel no-counterexample 
interpretation (called `metastability' by Tao) of $(x_n)$, namely a functional 
$\Phi(k,g)$ such that 
\[ (4) \ \forall k\in\NN\,\forall g:\NN\to\NN\, \exists n\le \Phi(k,g) 
\,\forall i,j \in [n;n+g(n)]\ \big( d(x_i,x_j)\le 2^{-k}\big). \]
Note that the underlying reformulation 
\[ (5) \ \forall k\in \NN\,\forall g:\NN\to\NN\, \exists n
\,\forall i,j \in [n;n+g(n)]\ \big( d(x_i,x_j)\le 2^{-k}\big) \]
of the Cauchy property 
still expresses the full Cauchy property of $(x_n).$ However, the proof 
of the latter from the former is ineffective corresponding to the fact 
there is no way to 
pass (even pointwise let alone uniformly) 
from an effective $\Phi$ in $(4)$ to an 
effective bound on fluctuations $F$ or an effective rate of convergence $\rho.$
\\[2mm] General logical metatheorems for strong systems of analysis based 
on full classical logic guarantee that the extractability of a (sub-)recursive 
(and highly uniform) rates of metastability $\Phi$ is always possible for 
large classes of convergence proofs. This has been applied extensively in 
the context of nonlinear analysis, fixed point theory and ergodic theory 
during the last 10 years. One of these results is the extraction of a 
uniform rate of metastability for the strong convergence in the mean 
ergodic theorem for uniformly convex Banach spaces $X$ carried out in 
\cite{Kohlenbach/Leustean4}. That a computable rate of convergence (even 
for an effective Hilbert space and a computable operator) in general 
is impossible has been shown in \cite{Avigad/Gerhardy/Towsner}. However, 
as recently observed by Avigad and Rute \cite{Avigad/Rute}, the analysis 
in \cite{Kohlenbach/Leustean4} can be used to obtain a simple effective (and 
also highly uniform) 
bound on the number of fluctuations (for the case of Hilbert spaces this 
was already obtained with an even better bound in \cite{Jones}). This 
raises the question whether there are general logical conditions on 
convergence proofs to guarantee the extractability of effective bounds 
on fluctuations. Obviously, any condition guaranteeing the extractability of 
a computable rate of convergence is a sufficient condition for this. Though 
not satisfied in the particular case just discussed, let us first consider 
this in order to see in what sense we might try to liberalize such conditions 
towards rates of fluctuations. To discuss things in somewhat more precise 
terms we fix a formal framework such as intuitionistic arithmetic 
HA$^{\omega}$ in all finite types (actually we use the so-called weakly 
extensional variant called WE-HA$^{\omega}$ in \cite{Kohlenbach(book)}) 
or its extension by an abstract (metric or) 
normed space $(X,\|\cdot\|)$ resulting 
in HA$^{\omega}[X,\|\cdot\|]$ possibly with further axioms stating that 
$X$ is uniformly convex or even a Hilbert space (see \cite{Kohlenbach(book)} 
for details). 
As follows from the bound extraction theorem for monotone modified 
realizability from \cite{Kohlenbach(book)} (and for theories with 
abstract spaces $X$ in \cite{GerKoh06}) from a proof of $(1)$ (for 
some sequence $(x_n)$ definable by a term $t$ of the system having at most 
number and function parameters $a,f$) in 
\[ \mbox{HA$^{\omega} +$AC$+$LEM}_{\neg} \] 
(and in fact even stronger theories augmented with certain ineffective 
axioms $\Omega$), the extractability of a rate of convergence $\rho$ that 
is definable (in the same parameters as $t$) 
G\"odel's calculus of primitive recursive functionals of 
finite type is guaranteed.  Here AC is the full schema of choice and 
LEM$_{\neg}$ is the law-of-excluded-middle schema restricted to arbitrary 
negated formulas $\neg\psi$ (which, in particular, includes the case 
of existential-free formulas and so as a very special case $\Pi^0_1$-LEM, 
i.e. LEM restricted to $\Pi^0_1$-formulas). In the case of 
HA$^{\omega}[X,\| \cdot\|]$ even parameters of types such as $X, \NN\to X, 
X\to X$ are allowed in the definition of the sequence $(x_n)$ in $X$ where 
then $\rho$ depends additionally on majorants for these parameters (which are 
natural numbers, in the case of the type $X,$ and number-theoretic 
functions, in the case of the types $\NN\to X$, $X\to X.$) \\[2mm] 
An important weak principle of classical logic not covered by this is 
the so-called Markov principle which, extended to all finite types, reads 
as follows 
\[ \mbox{M}^{\omega}\ :\ 
\neg\neg\exists \underline{x}^{\underline{\rho}} \ \varphi_0(\underline{x})\to 
\exists \underline{x}^{\underline{\rho}}\,\varphi_0(\underline{x}), \] 
where $\varphi_0$ is a quantifier-free formula and $\underline{\rho}$ an 
arbitrary tuple of types. However, M$^{\omega}$ becomes permissible once 
LEM$_{\neg}$ is weakened to the so-called lesser-limited-omniscience-principle 
LLPO (which is the precise amount of classical logic needed to prove the 
binary (`weak') K\"onig's lemma WKL; see \cite{Kohlenbach(book)} for details).
So instead of HA$^{\omega}([X,\|\cdot\|])+$AC$+$LEM$_{\neg}$ we may also have 
\[ \mbox{HA$^{\omega}([X,\|\cdot \| ])+$AC$+$M$^{\omega}+$LLPO}, \] 
where then the extraction of a rate of convergence uses the so-called 
monotone functional interpretation (see \cite{Kohlenbach(book)}).
\\[2mm] The in a sense weakest principle covered by neither of these systems 
(but provable in their union!) is LEM restricted to $\Sigma^0_1$-formulas, 
which we denote by $\Sigma^0_1$-LEM: 
\[ \Sigma^0_1\mbox{-LEM}: \ \ \exists n\in\NN \,\varphi_0(n)\vee
\forall m\in\NN\,\neg \varphi_0(m), \]
where $\varphi_0$ is quantifier-free (but may contain parameters of 
arbitrary type). \\  
While $\Sigma^0_1$-LEM in the presence of 
AC (even when restricted to numbers) 
creates highly noncomputable functions (in particular when function 
parameters are allowed to occur in $\Sigma^0_1$-LEM which then makes it 
possible to climb up the entire arithmetical hierarchy) it remains fairly 
weak over HA$^{\omega}.$ Nevertheless HA$+\Sigma^0_1$-LEM 
already allows 
to prove the Cauchyness of the Specker sequence, a primitive recursive 
monotone decreasing sequence of rational numbers in $[0,1]$ which does not 
have a computable rate of convergence. In fact, as shown in \cite{Toftdal}, 
the principle that every bounded monotone sequence of reals is Cauchy can be 
proven HA$^{\omega}+\Sigma^0_1$-LEM. This is not obvious and requires a novel 
proof as the usual argument uses the (by \cite{Akama} e.g. over HA) 
strictly stronger principle 
\[ \Sigma^0_2\mbox{-DNE}\ :\ \neg\neg\exists n\in\NN\,\forall m\in\NN\,
\varphi_0(n,m)\to\exists n\in\NN\,\forall m\in\NN\,\varphi_0(n,m) \]
(`double-negation-elimination principle' for 
$\Sigma^0_2$-formulas) While $\Sigma^0_2$-DNE is limit computable in the 
sense of Hayashi and 
Nakana \cite{Hayashi/Nakata}, 
any single instance of $\Sigma^0_1$-LEM is even learnable 
with a single mind change. Note also that bounded monotone sequences of 
real numbers (say in $[0,1]$) always have the simple fluctuation bound 
$F(k):=2^k.$ That $\Sigma^0_1$-LEM$^-$ has a strictly stronger 
computational interpretation than $\Sigma^0_2$-DNE$^-$ can be spelled 
out in terms of proof interpretations:  $\Sigma^0_1$-LEM$^-$ (when added 
e.g. to HA$^{\omega}$) admits  
a modified realizability interpretation 
by terms that are primitive recursive (in the 
sense of G\"odels $T$) relative to a Skolem function $f_{\varphi}$ for 
\[ \forall k\in\NN\exists n \in\NN\forall m\in\NN 
\ (\varphi_0(k,n)\vee\neg\varphi_0(k,m)) 
\]  
and much work studying this interpretation in terms of learning 
theory and so-called 1-backtracking games (in the sense of 
Coquand's game semantics \cite{Coquand,Berardi/Coquand/Hayashi}) 
has been carried out in recent years e.g. by 
Aschieri and Berardi (see e.g. 
\cite{Aschieri/Berardi,Aschieri1,Aschieri2}). By contrast to this, 
$\Sigma^0_2$-DNE$^-$ does not allow such an interpretation and not 
even a (in this context) weaker monotone modified realizability 
interpretation (in the sense of the \cite{Kohlenbach(relative)}, 
see also \cite{Kohlenbach(book)}) as shown in \cite{Akama} where this 
is used to prove that $\Sigma^0_2$-DNE$^-$ not even follows from 
$\Pi^0_2$-LEM$^-$. \\[2mm]
All this at a first look might suggest to consider 
HA$^{\omega}+\Sigma^0_1$-LEM$^{(-)}$ as a promising framework to 
guarantee (while in general not computable rates of convergence) 
computable bounds on the number of fluctuations for provable Cauchy 
sequences. However, this 
turns out to be mistaken as $\Sigma^0_1$-LEM is already the general case: 
let $(x_n)$ be a sequence of real numbers definable by a term $t$ 
in HA$^{\omega}$ 
(which may have variables of arbitrary type as parameters). Suppose that 
\[ \mbox{PA}^{\omega}\vdash \forall k\,\exists n\,\forall m,\tilde{m}\ge n\,
(|x_m-x_{\tilde{m}}|\le_{\RR} 2^{-k}). \]
Then by negative translation (see \cite{Kohlenbach(book)}) 
\[ \mbox{HA}^{\omega}\vdash  \forall k\,\neg\neg 
\exists n\,\forall m,\tilde{m}\ge n\,
(|x_m-x_{\tilde{m}}|\le_{\RR} 2^{-k}). \]
Adapting Friedman's proof for the closure of HA$^{\omega}$ under the Markov 
rule one can show (this is stated for HA without proof in 
\cite{Hayashi/Nakata} and we include a proof -- also for 
HA$^{\omega}[X,\|\cdot\|]$ -- below) that HA$^{\omega}+
\Sigma^0_1$-LEM  
is closed under the rule version of $\Sigma^0_2$-DNE so that we get 
\[ \mbox{HA$^{\omega}+\Sigma^0_1$-LEM} \ 
\vdash \forall k\,\exists n\,\forall m,\tilde{m}\ge n\,
(|x_m-x_{\tilde{m}}|\le_{\RR} 2^{-k}). \] 
Moreover, if $t$ contains at most number parameters it also suffices to 
use the restriction $\Sigma^0_1$-LEM$^-$ of $\Sigma^0_1$-LEM to 
$\Sigma^0_1$-formulas with number parameters only. All this also holds for 
the systems HA$^{\omega}[X,\|\cdot\|]$ and PA$^{\omega}[X,\|\cdot\|]$ 
and sequences $(x_n)$ in $X$ defined by terms of these systems.
\\[2mm] 
So, as far as $\Pi^0_3$-theorems are concerned 
(also with parameters in $\NN,\NN^{\NN}$ 
or -- in the case of theories with an abstract normed space $X$ -- 
also in $X,X\to X, \NN\to X$), there is no difference in proofs based 
on full classical logic versus proofs using only $\Sigma^0_1$-LEM 
(at least as long the proofs can be formalized in systems to which 
negative translation and Friedman's $A$-translation apply). So in 
order to have a computational content stronger than metastability 
guaranteed we have to look for more restricted uses of $\Sigma^0_1$-IA.  
\\  
Looking more carefully into the $\Sigma^0_1$-LEM based proof of the 
Cauchyness of bounded monotone sequences as given in \cite{Toftdal} reveals 
that one can define a sequence of instances $\Sigma^0_1$-LEM$(s(n))$) 
of $\Sigma^0_1$LEM$^-$ 
\[ \exists m\in\NN \,(s(n,m)=0)\vee \forall m\in\NN\,(s(n,m)\not= 0) \] 
such that to prove the Cauchy property with error $2^{-k}$ one only needs 
the first $n=0,\ldots,s(t(k))$-many instances of this sequence where $t$ is 
a simple primitive recursive function. So a more promising approach would be
to look at proofs of Cauchy statements which can be formalized as follows:
\[ \mbox{HA}^{\omega}\ \vdash \forall k \  \big( \forall l\le t(k) \ 
\Sigma^0_1\mbox{-LEM}(s(l))\to \exists n\,\forall m,\tilde{m}\ge n\,
(|x_m-x_{\tilde{m}}|\le_{\RR} 2^{-k})\big) \] 
or 
\[ \mbox{HA}^{\omega}[X,\|\cdot\|] \ \vdash \forall k \  \big( \forall l\le 
t(k) \ 
\Sigma^0_1\mbox{-LEM}(s(l))\to \exists n\,\forall m,\tilde{m}\ge n\,
(\|x_m-x_{\tilde{m}}\|\le_{\RR} 2^{-k})\big), \]
where $t$ may contain parameters of type $\NN, \NN\to\NN$ (and $X,\NN\to X, 
X\to X$ in the extended system). \\[2mm] 
We show that from such proofs one can always extract effective (and in 
fact primitive recursive in the sense of G\"odel's $T$) bounds $B,L$ on the 
effective learnability of a rate of convergence of $(x_n).$ Here 
$B,L$ are effective functionals (in the parameters of the problem) where 
$L$ is the learning procedure and $B$ a bound on the number of 
necessary steps along this procedure.  
This is, as we will show, a strictly stronger information than a 
rate of metastability as 
the latter can be obtained from (majorants $B^*,L^*$ of) the former 
even by a uniform primitive 
recursive procedure (in the ordinary sense of Kleene) but there are 
primitive recursive 
Cauchy sequences with a primitive recursive (again in the ordinary 
sense of Kleene) rate of metastability which 
do not admit any computable bound for the 
learnability of a rate of convergence. \\ In fact, the additional 
information provided by $B^*,L^*$ becomes visuable by the particular 
simple structure of the rate of metastability obtained from 
$B^*,L^*$ which is guaranteed 
to be of the form (essentially) 
\[ (L^*(\underline{a}^*)\circ g)^{(B^*(\underline{a}^*))}(0), \] 
where $f^{(x)}(0)$ denotes the $x$-times iteration of the function $f$ 
starting from $0.$ The essential point here is that $B^*,L^*$ do not 
involve the counterfunction $g.$ It is precisely this form of a rate 
of metastability that has been observed many times in concrete unwindings 
in ergodic theory and fixed point theory (see e.g.\cite{Avigad/Gerhardy/Towsner,Kohlenbach/Leustean4,Kohlenbach/Leustean3,Kohlenbach(Browder),Kohlenbach/Leustean6,Kohlenbach/Leustean7}) and which we can explain 
now for the first time in terms of the logical structure of the given proof. 
Notable exceptions to this restricted format of metastability are the rates 
of metastability for 
the ergodic theorem for odd (and even more general) operators in 
\cite{Safarik(11)} (making a nested use of the iteration procedure) and 
for the (weak convergence in the) Baillon nonlinear ergodic theorem 
in \cite{Kohlenbach(Baillon)} (making even nested use of a bar recursive 
functional). However, both underlying proofs precisely 
violate our criterion of a bounded use of $\Sigma^0_1$-LEM.  
\\[2mm]
A bound of the number of fluctuations in general is a still  
strictly stronger quantitative information than bounds on the effective 
learnability: we construct a primitive recursive sequence in $\RR$ which 
has primitive recursive bounds on the learnability of its Cauchy rate 
but does not admit a computable bound on the number of its fluctuations.
Together with the already discussed Specker sequences (which have 
trivial fluctuation bounds but no effective rates of convergence) 
we get the (w.r.t. effectivity) strictly decreasing hierarchy of 
quantitative data for the convergence of Cauchy sequences $(x_n)$:
\begin{enumerate}
\item 
A rate $\rho$ of convergence of $(x_n).$
\item 
A bound $F$ on the number of approximate fluctuations of $(x_n).$
\item 
Function(al)s $B,L$ (see the next section for a precise definition) 
of the learnability of a rate of convergence for $(x_n)$ by 
$B$-many mind changes by a learning procedure $L.$
\item 
A rate $\Phi$ of metastability for $(x_n).$
\end{enumerate}   
While -- as discussed above -- the extractability of effective data 
for the levels 1, 3 and 4 of this hierarchy is guaranteed by 
relatively easy to check logical {\bf a-priori}  
conditions on the framework in which a Cauchy statement is proven, 
this seems to be different for level 2: we give a kind of gap condition 
to be satisfied by $L$ in `3.' which suffices to convert the 
information provided by $B,L$ into a bound $F$ on the fluctuations. 
Since to check this condition requires the inspection of the extracted 
data $B,L$ this is an {\bf a-postiori} condition.
\\[2mm] 
The above hierarchy apparently fits well to distinguish the 
computational content that recently has been obtained from proofs 
in the context of ergodic theory: \\[1mm] 
As discussed already, Avigad and Rute \cite{Avigad/Rute} observed 
that the extraction of a rate of metastability from Birkhoff's 
proof of the mean ergodic theorem in uniformly convex Banach 
spaces carried out in \cite{Kohlenbach/Leustean4} can in fact been 
used to even obtain a uniform effective fluctuation bound 
This corresponds to second level of our hierarchy 
which by \cite{Avigad/Gerhardy/Towsner} 
cannot be further improved effectively to the first level.
\\[1mm] The logical condition needed to assure the extractability of a 
primitive recursive 
(in the ordinary sense of Kleene) data $B,L$ for the third 
level is e.g. satisfied in 
the proofs of the strong convergence of so-called Halpern iterations 
in Hilbert spaces (due to Wittmann \cite{Wittmann(92)}) and -- more 
generally --  in CAT(0) spaces (due to Saejung in 
\cite{Saejung}). This follows from the 
analysis of Saejung's proof 
given in \cite{Kohlenbach/Leustean6} where a primitive 
recursive (in the ordinary sense) rate of metastability is extracted
(the analysis of Wittmann's proof in \cite{Kohlenbach(Browder)} also 
shows this in the Hilbert case).  
A special form of the Halpern iteration (covered by 
\cite{Wittmann(92),Saejung}) is given by  
\[ x_{n+1} :=\frac{1}{n+2}x_0+\left(1-\frac{1}{n+2}\right) Tx_n \]
can be viewed as a nonlinear generalization of 
the ergodic average 
\[ \frac{1}{n+1}\sum^n_{i=0}T^ix_0 \] 
in the mean ergodic theorem with which it coincides 
for {\bf linear} nonexpansive  maps $T.$ 
As a corollary we obtain the extractability even 
of primitive recursive learnability data $B,L$ in this case. 
However, the aforementioned gap condition does not seem to be 
satisfied and so, as it stands, a fluctuation bound does not seem 
to follow.
\\[1mm]  
While the strong convergence of the ergodic average in general is known 
to fail for nonlinear nonexpansive maps (whereas weak convergence 
holds by a deep theorem of Baillon \cite{Baillon(75)}), 
it does hold e.g. 
for odd operators as shown again by Baillon \cite{Baillon(76)} 
and -- for a much more 
general class of mappings -- by Wittmann \cite{Wittmann(90)}. 
Recently, the second author 
\cite{Safarik(11)} extracted 
a (primitive recursive in the sense of Kleene) rate of metastability 
(i.e. level 4) from Wittmann's proof. However, Wittmann's proof does 
not satisfy the condition sufficient to guarantee level-3 learnability 
data and, in fact, the extracted bound has the same structure as the 
one in our example of a primitive recursive sequence separating the 
levels 3 and 4 given section 4 below. 



\section{Fluctuations versus effective learnability}
To be specific, let us use the following we the language of (intuitionistic) 
arithmetic in all finite types HA$^{\omega}$ (more precisely the system 
WE-HA$^{\omega}$, see \cite{Kohlenbach(book)}) as well as its extension HA$^{\omega}[X,\|\cdot\|]$ 
by an abstract normed space $X$ in the sense of \cite{Kohlenbach(metapaper),GerKoh06,Kohlenbach(book)} in order to be able to cover also the aforementioned 
recent applications of proof mining to ergodic and fixed point theory which 
need this enriched language. Everything we say extends mutatis mutandis also 
to theories where more structure of $X$ is prescribed (e.g. $X$ being 
a uniformly convex or Hilbert space) and convex subsets $C$ of $X$ being 
added as well as to metric structures $X$ 
such as metric, $W$-hyperbolic and CAT(0) spaces (see \cite{Kohlenbach(book)} 
for all this). The type of natural numbers $\NN$ is usually denoted by $0.$
\\[1mm] ${\cal S}^{\omega,X}$ denotes the full set-theoretic model of these 
theories over the base types $\NN$ and $X.$ Occasionally, we will need 
the relation `$x^*$ majorizes $x$' (short: $x^* \ maj \ x$) due to W.A. Howard 
(for the finite types over $\NN$)  
which is defined in the usual hereditary 
way by induction on the type of $x$ starting from 
\[ x^* \ maj_0 \ x\ :\equiv x^*\ge x \] for $x^*,x$ of type $0$ and 
\[ x^* \ maj_X \ x\ :\equiv x^* \ge \| x\|\] for $x$ of type $X$ and 
$x^*$ of type $0.$ 

%In proof mining in general, one extracts effective (uniform) bounds for witnesses for certain existential statements.
%In case of Cauchy statements this could be an effective (uniform) bound for
%\begin{enumerate},
%	\item the rate of convergence, \label{e:conv}
%	\item the number of $\epsilon$-fluctuations, \label{e:fluc}
%	\item the number of mind changes to learn the rate of convergence, \label{e:fmc}
%	\item the rate of metastability. \label{e:meta}
%\end{enumerate}

\begin{dfn}[the number of fluctuations]
For a sequence $x_{(\cdot)}$ in some metric space $(X,d)$ and an $\epsilon>0$
let $\Fluc(n,i,j)$ denote that there are $n$ fluctuations whose indexes are encoded into $i$ and $j$.
\begin{align*}
\Fluc(n,i,j):\equiv\Fluc_{x_{(\cdot)},\epsilon}(n,i,j):\equiv\quad &\lh(i)=\lh(j)=n\quad \wedge\\ 
&\forall k<n\ (i_k<j_k) \quad \wedge\\
&\forall k<n-1\ (j_k\leq i_{k+1}) \quad \wedge\\
&\forall k<n\ (d(x_{i_k},x_{j_k})>\epsilon).
\end{align*}
We call $b$ a bound on the number of $\epsilon$-fluctuations of $x_{(\cdot)}$, iff 
\[
\forall n>b \forall i,j \neg\Fluc(n,i,j).
\]
We call $b$ effective if it is computable in $\epsilon\in\QQ^*_+$ and $x_{(\cdot)}$.
\end{dfn}



In the Language identification in the limit model for inductive inference, the notion of learnable with an existence of a mind change bound was introduced in the sixties \todo{reference!}. We define a similar concept in the context of general formal theories like $\pa$. Since we require both the learning procedure and the bound on mind changes to be recursive (effective) we call this property {\em effective learnability}.\\
Before we consider our most general definition, let us consider learnability of monotone formulas (which is a rather rich class of statements including in particular Cauchy statements), since in this case, the definition is not only extremely intuitive, but also fairly definite.

% 
%	fmcMon
%
\begin{dfn}[ B,L-learnable monotone formulas]\label{d:fmcMon}
Consider a $\Sigma^0_2$ formula $\phi$ with the only parameters $\tup a^{\tup \sigma}$, i.e.
\[\phi\ \equiv\ \exists n^0 \forall x^0\ \phi_0(x,n,\tup a),\]
which is monotone in $n$, i.e.
\[ 
\forall n^0\ \forall n'\ge n\ \forall x^0\ \big(  \phi_0\ (x,n,\tup a) \rightarrow  \phi_0(x,n',\tup a) \big).
\]
We call such a formula $\phi$ 
{\em (B,L)-learnable},
if there are function(al)s $B$ and $L$ such that the following holds 
(in the full set-theoretic model ${\cal S}^{\omega,X}$):
\[ 
\exists i\leq B(\tup a)\ \forall x\ \phi_0(x,c_i,\tup a),\] where
\begin{align*}
c_0&:=0,\\
c_{i+1}&:=
\begin{cases}
L(x, \tup a),&\text{for the $x$ with } \neg\phi_0(x,c_i,\tup a)\ \wedge\ \forall y<x\ \phi_0(y,c_i,\tup a) \text{ if it exists}\\
c_i,&\Telse.
\end{cases}
\end{align*}
%\[ \forall x,i\quad \Big(\ \big(\neg\phi_0(x,c_i,\tup a)\ \wedge\ \forall y<x\ \phi_0(y,c_i,\tup a)\big) \rightarrow\ G(x, c_{i},\tup a) = c_{i+1}\ \Big). \]
We call such a $\phi$ 
{\em effectively learnable with finitely many mind changes (short: fmc)},
if it is $(B,L)$-learnable, $\sigma_i$ has type level at most one, and $B$ and $L$ are computable.
\end{dfn}


This definition is very intuitive in the sense that it formalizes the concept of an (effective) learning process $L$ which learns the witness in an effectively bounded number of attempts in a very straightforward way.\\
Moreover, this definition allows the learner, i.e. the function $L,$ to use the least amount of non-computable information possible, namely only the smallest counterexample to the learners last candidate for the witness. 
Nevertheless, we will show that this amount of information is, in a sense, 
already exhaustive. More precisely, we have the following:
\begin{prop}\label{p:allx}
Consider a monotone formula $\phi$ as above. Suppose there are $B$ and $L'$ s.t.
\[ \exists i\leq B(\tup a)\ \forall x\ \phi_0(x,c'_i,\tup a),\] where this 
time $L'$ can access all reasonable information, i.e.
\begin{align*}
c'_0&:=0,\\
c'_{i+1}&:=
\begin{cases}
L'(\langle x_0,\ldots,x_i\rangle, \langle c'_0,\ldots,c'_i\rangle,\tup a), &\text{for those $x_j,c'_j$, $j\leq i$ with }\\
 &\neg\phi_0(x_j,c'_j,\tup a)\ \wedge\ \forall y<x_j\ \phi_0(y,c'_j,\tup a)\\
  &\text{ if each exists},\\
c'_i, &\Telse.
\end{cases}
\end{align*}
Then $\phi$ is $(B,L)$-learnable in the original sense (as defined in Definition~\ref{d:fmcMon}) and $L$ is primitive recursively definable in $L'$.
\end{prop}
\begin{proof}
W.l.o.g we can assume that there is a $j\leq B'(\tup a)$ s.t. $\forall i<j\ (c_{i+1}>c_i)$ and $\forall i\geq j\ (c_{i+1}=c_i)$ (we can actually primitive recursively define a learner which satisfies this property whenever we have $B'$ and $L'$ as above). 
%We set
%\[
%L^*(\langle x_0,\ldots,x_i\rangle, \langle \tup c\rangle,\tup a)
%:=\min\{l'> L'(\langle x_0,\ldots,x_i\rangle, \langle \tup c\rangle,\tup a)
%\ :\ \forall j\leq i\ \phi_0(x_j,l',\tup a)\},
%\]
%such a minimum always exists since $\phi$ has to be valid (otherwise it couldn't be $B',L'$-learnable in the first place) and use
We set
\begin{align*}
L(x, \tup a):=L(\langle\rangle,\langle\rangle,x, \tup a)&:=\begin{cases}
0,&\Tif \phi_0(x,0,\tup a),\\
L( \langle X(x,0,\tup a)\rangle,\langle0\rangle,x,\tup a),&\Telse,\end{cases}\\
L(\langle \underbrace{x_0,\ldots,x_i}_{\tup x}\rangle, \langle \underbrace{c_0,\ldots,c_i}_{\tup c}\rangle,x,\tup a)&:=\begin{cases}
c_i,&\Tif \bigvee_{j\leq i}\phi_0(x_j,c_j,\tup a),\\
l':=L'(\langle {\tup x}\rangle, \langle {\tup c}\rangle,\tup a),&\Tif x=x_i\vee \phi_0(x,l',\tup a),\\
&\phantom{\Tif} \bigwedge_{j\leq i}\neg\phi_0(x_j,c_j,\tup a),\\
L( \langle {\tup x},X(x,l',\tup a)\rangle,
\langle \tup c,l'\rangle,x,\tup a) &\Telse,
%L( \langle {\tup x},X(x,l',\tup a)\rangle,
%\langle \tup c,l'\rangle,\tup a) &\Tif x\neq x_i,\neg\phi_0(x,l',\tup a), \bigwedge_{j\leq i}\neg\phi_0(x_j,c_j,\tup a),
\end{cases}\\
\end{align*}
where $X(x,c,\tup a):=\min\{x'\leq x\ :\ \neg \phi_0(x',c,\tup a)\}$ (or $x$ if there is no such $x'$).\\
{\em We show by induction on $i$ that $\forall i\ (c'_i = c_i)$.} This is obvious for $i=0$, moreover 
if $\forall x\ \phi_0(x,c'_{i},\tup a)$ then also $\forall x\ \phi_0(x,c_{i},\tup a)$ and so $c'_{(\cdot)}=c_{(\cdot)}$ (both) by the induction hypothesis.\\ Otherwise, we have the smallest counterexample $x'_i$ to $c'_i$, since by our hypothesis $c'_i=c_i$ we have also $x_i=x'_i$ for the smallest counterexample to $c_i$. 
So, by monotonicity of $\phi$ we obtain for all $j<i$ that $x'_j\leq x_i$, so $X(x_i,c'_j,\tup a)=x'_j$ and by definition of $L$ we get in total that 
\begin{align*}
c_{i+1}=L(x_i,\tup a)&=L(\langle X(x_i,0,\tup a)\rangle,\langle0\rangle,x_i,\tup a)\\
&=L(\langle x'_0\rangle,\langle0\rangle,x_i,\tup a)\\
&=L(\langle x'_0,  X(x_i,   L'(\langle x'_0\rangle,\langle0\rangle,\tup a)    ,\tup a)\rangle,\langle0, 
     L'(\langle x'_0\rangle,\langle0\rangle,\tup a)  \rangle,x_i,\tup a)\\
&=L(\langle x'_0,  X(x_i,   c'_1    ,\tup a)\rangle,\langle0, 
     c'_1  \rangle,x_i,\tup a)\\
&=L(\langle x'_0,  x'_1\rangle,\langle0, 
     c'_1  \rangle,x_i,\tup a)\\
&=\ldots\\
&=L'(\langle\tup x'\rangle,\langle\tup c'\rangle, \tup a)=c'_{i+1}.
\end{align*}

%{\em We show by induction on $i$ that $\forall i\ \forall j\leq i c'_i\leq c_i$.} This is obvious for $i=0$, moreover
%if $\forall x\ \phi_0(x,c'_{i},\tup a)$ then also $\forall x\ \phi_0(x,c_{i},\tup a)$ and so $c'=c$ (both) by the induction hypothesis. Otherwise, we have the smallest counterexamle $x'_i$ to $c'_i$, again by our hypothesis i.e. $c'_i=c_i$ and therefore we have also $x=x'_i$. Furthemore we obtain by the monotonicity that:
%\begin{align*}
%c_i=L(x,\tup a)=L(
%c'_{i+1}=L'(\langle\tup x'\rangle,\langle\tup c'\rangle, \tup a)=L'(\langle x_0,\ldots,x_i\rangle)
%\end{align*}

\end{proof}

So from now on we will simply use $L$ in the form which suits us best.

\begin{rmk}\label{r:smallestCE}
In Definition~\ref{d:fmcMon}, even the condition that $x$ is the smallest counterexample, i.e. $\forall y<x\ \phi_0(y,c_i,\tup a)$, isn't really necessary. Of course, then we need $B$ to be a bound for any such sequence $c_{(\cdot)}$,\footnote{The alternative, letting $B$ be a bound for just some sequence makes little sense, since if there was any bound and learner at all, then $B(\tup a)=1$ would be a correct bound for the same learner as well.} since it is not unique anymore (not in general). However, such a definition would be equivalent:
\begin{itemize} 
\item Any given bound $B$ for all such sequences is obviously in particular a bound for the one we used in the original definition.
\item On the other hand, given $B$ and $L$ satisfying our original definition, we could modify $L$ in such a way, that it looks for the smallest counterexample and uses that for its computation, assuring that we actually generate the same sequence $c_{(\cdot)}$ after all (e.g. set 
$L'(x,c,\tup a):=L(\min \{x'\leq x\ :\  \neg\phi_0(x',c,\tup a)\ \wedge\ \forall y<x'\ \phi_0(y,c,\tup a)\},c,\tup a)$ if such an $x'$ exists and 
$L'(x,c,\tup a):=L(x,c,\tup a)$ otherwise).
\end{itemize}
\end{rmk}

As far as monotone formulas are concerned we have yet another nice property.
\begin{prop}\label{p:majBL}
A monotone $\Sigma^0_2$-formula $\phi$ (see also Definition~\ref{d:fmcMon}) that is $B,L$-learnable (uniformly in the parameters $\underline{a})$ 
is also $(B^*,L^*)$-learnable (uniformly in majorants $\tup a^*$ of 
$\tup a$) for any majorants $B^*$,$L^*$ of $B,L,$ i.e. 
(in ${\cal S}^{\omega,X}$)  
\[ \forall 
\underline{a}^*,\underline{a} \big( 
\underline{a}^* \ \maj\ \underline{a}\to 
\exists i\leq B^*(\tup a^*)\ \forall x^0\ \phi_0(x,c^*_i,\tup a)\big),\] where
\begin{align*}
c^*_0&:=0,\\
c^*_{i+1}&:=
\begin{cases}
L^*(x, \tup a^*),&\text{for the $x$ with } \neg\phi_0(x,c^*_i,\tup a)\ \wedge\ \forall y<x\ \phi_0(y,c^*_i,\tup a) \text{ if it exists}\\
c^*_i,&\Telse.
\end{cases}
\end{align*}
\end{prop}
{\bf Proof.} Note that $B^*,L^*,\tup a^* \ maj \ B,L,\tup a$ implies that 
\[ B^*(\underline{a}^*) \ge B(\underline{a}) \,\wedge\,\forall x^0,y^0 
(x\ge y\to L^*(x,\underline{a}^*)\ge L(y,\underline{a}))).\] 
Moreover, using this we have by the monotonicity of $\varphi$ that for any $x,x'$
\[ \neg \phi_0(x',L^*(x,\underline{a}^*),\tup a) \rightarrow \neg \phi_0(x',L(x,\underline{a}),\tup a), \]
(which means, in particular, that the smallest counterexample to $L(x,\underline{a})$ has to be smaller than (/equal to) any (/the smallest) counteraxample for $L^*(x,\underline{a}^*)$),\\
so inductively we obtain that $\forall i\ (c^*_i\geq c_i)$. \hfill $\Box$
\begin{rmk} If the parameters $\underline{a}$ have all types of degree 
$\le 1$, then $\varphi$ in the above proposition is learnable in $B^*,L^*$ 
uniformly in $\underline{a}$ since $a^M \ maj \ a^1,$ where 
$a^M(n):=\max\{ a(i):i\le n.\}.$
\\ In this sense, we can extend the term effectively learnable as follows. A monotone $\Sigma^0_2$-formula $\phi(\tup a)$ is 
{\em effectively learnable with finitely mind changes} if it is 
$B^*,L^*$-learnable (uniformly in majorants $\underline{a}^*$ of the 
parameters $\underline{a}$ by computable functionals $B^*,L^*$ and all elements of $\tup a^*$ are of type level at most one and $B^*$,$L^*.$ 
Note that this means that in the System $\ha[X,\|\cdot\|]$, $\tup a$ could include parameters of types like $X,\NN\to X,X\to X$.
\end{rmk}

So speaking of a Cauchy sequence $a_{(\cdot)}$, we would say that it has an effectively learnable rate of convergence, if there is a recursive computation for a bound $b$ from the sequence $a_{(\cdot)}$ and a positive $\epsilon$, such that there is a procedure to learn an $\epsilon$-Cauchy point with at most $b$ computable corrections (computable in a counterexample $x$, which in turn may not be computable itself!).\\

There are several ways to generalize our learnability definition. 
Of course one can drop the monotonicity condition, but we can also allow higher or abstract types for $n$ and $x$ (for $x$ we need to consider all sequences $c_{(\cdot)}$ since we can provide only a counterexample $x$, not the smallest counterexample $x$ -- see also Remark~\ref{r:smallestCE}). The question here is, what kind of information do we allow the learning function(al) $L$ to access. At the moment, it seems that there is not such a nice and definitive answer as in the monotone case. However, we will stick with a (not necessarily unique) definition, which
\begin{itemize} 
\item fits very nicely into the hierarchy of different concepts for computational information (see Proposition~\ref{p:bg2meta}),
\item makes effective learnability guaranteed by very clear logical conditions on the provability of the learned formula (see Theorem~\ref{t:bdLem}),
\end{itemize} 

% 
%	fmcNum
%
\begin{dfn}[(B,L)-learnable formulas over higher type objects]\label{d:fmcNum}
Consider an $\exists\forall$ formula $\phi$ with the only parameters $\tup a^{\tup \sigma}$, i.e.
\[\phi\ \equiv\ \exists n^\rho \forall x^0\ \phi_0(x,n,\tup a).\]
We call such a formula $\phi$ 
{\em B,L-learnable},
if there are function(al)s $B$ and $L$ such that the following holds:
\[ \exists i\leq B(\tup a)\ \forall x\ \phi_0(x,c_i,\tup a),\] where
\begin{align*}
c_0&:=0,\\
c_{i+1}&:=
\begin{cases}
L(\langle x_0,\ldots,x_i\rangle, \tup a), &\text{for those $x_j$, $j\leq i$ with }\\
 &\neg\phi_0(x_j,c_j,\tup a)\ \wedge\ \forall y<x_j\ \phi_0(y,c_j,\tup a)\\
  &\text{ if each exists},\\
c_i, &\Telse.
\end{cases}
\end{align*}%\[ \forall x,i\quad \Big(\ \big(\neg\phi_0(x,c_i,\tup a)\ \wedge\ \forall y<x\ \phi_0(y,c_i,\tup a)\big) \rightarrow\ G(x, c_{i},\tup a) = c_{i+1}\ \Big). \]
We call such a $\phi$ {\em effectively learnable}, if it is $(B,L)$-learnable,
$\sigma_i$ and $\rho$ have type level at most one, and $B$ and $L$ are computable.
\end{dfn}
\newcommand{\seq}{_{(\cdot)}}
\rmk{ Again, this definition already captures (so to say in a primitive recursive way) the case where the learner could access the previous values of $c\seq$ as well. Simply consider
\begin{align*}
&L(\overbrace{\langle x_0,\ldots,x_i}^{\tup x:=}\rangle, \tup a):=\\&\quad \quad
L'\big(\langle \tup x\rangle, \big\langle
 \underbrace{L'(\langle x_0\rangle, \langle 0\rangle,\tup a)}_{c'_1:=},
 \underbrace{L'(\langle x_0,x_1\rangle, \langle 0,c'_1\rangle,\tup a)}_{c'_2:=}, 
 \ldots,
 L'(\langle \tup x\rangle, \langle 0,c'_1,\ldots,c'_i\rangle,\tup a)
 \big\rangle, \tup a\big). 
\end{align*}
Of course, one could consider weaker concepts, like a learner which can access only the last counterexample (as in the monotone case). We considered also a learner of the kind $L'(x,c,\tup a)$ (i.e. a learner who is allowed to use in addition only the lastly learned solution candidate), which doesn't seem to be equivalent to any of the other two concepts.\\[2mm]
Let us make the properties of our learnability definition discussed above 
more transparent by proving the following results:
\\[1mm] 
In the following IP$^{\omega}_{\forall}$ denotes 
the independence-of-premise principle for universal 
premises in all finite types:
\[\mbox{IP}^{\omega}_{\forall}: \ \ (\forall \underline{x}\,A_0(\underline{x})
\to\exists y \,
B(y))\rightarrow \exists y\,( A_0(\underline{x})\to B(y)), \]
where $A_0$ is a quantifier-free formula and $\underline{x},y$ are 
variables of 
arbitrary types.
\begin{thm}\label{t:bdLem}
Given that
\[ \ba{l} 
\ha[X,\|\cdot\|]+\, \mbox{\rm AC$+$M$^{\omega}+$IP}^{\omega}_{\forall} 
\vdash\\[1mm] \hspace*{2cm} \forall \underline{a}\,\exists l^0\ \big( 
\forall m\leq_0 l \exists u^0 \forall v^0\ 
\big(\psi_0(u,m,\tup a) \vee \neg\psi_0(v,m,\tup a)\big)\rightarrow \exists 
n^0 \forall x^0 \phi_0(x,n,\tup a)\big),\ea 
\]
where $\phi_0,\psi_0$ are quantifier-free formulas (containing at most the 
parameters $\underline{a}$ free) and $t$ is a closed term in $\T$, and $\phi_0(x,n,\tup a)$ is monotone in $n$, i.e. that
\begin{align*} {\cal S}^{\omega,X} \models 
n'>n\ \rightarrow\ \big( \forall x \phi_0(x,n,\tup a) \rightarrow  \forall x \phi_0(x,n',\tup a) \big)
\end{align*}
then $\exists n\forall x \phi_0(x,n,\tup a)$ 
is (valid in ${\cal S}^{\omega,X}$) $B,L$-learnable by functionals given by 
closed terms of $\ha[X,\|\cdot\|].$ \\
To $B,L$ one can construct majorants 
$B^*,L^*$ given by closed terms of $\ha$ such that 
if $\exists n\forall x \phi_0(x,n,\tup a)$ is monotone (in the sense of Definition~\ref{d:fmcMon}), then it is even learnable in $B^*,L^*$ uniformly in majorants $\underline{a}^*$ of the parameters $\underline{a}.$
\end{thm}
\begin{proof}
Suppose that 
\[ \ba{l} 
\ha[X,\|\cdot\|]+\mbox{AC$+$M$^{\omega}+$IP}^{\omega}_{\forall} 
\vdash \\[1mm] \hspace*{2cm} 
\forall \underline{a}\exists l^0\ \big( 
\forall m\leq l \exists u \forall v\ \big(\psi_0(u,m,\tup a) \vee \neg\psi_0(v,m,\tup a)\big)\rightarrow \exists n \forall x \phi_0(x,n,\tup a)\big).
\ea \]
Then by the soundness of the G\"odel functional (`Dialectica') 
interpretation for $\ha[X,\|\cdot\|]+\mbox{AC$+$M$^{\omega}+$IP}^{\omega}_{\forall}$ (see \cite{Kohlenbach(book)}) we obtain that (note that since we do not need 
bar recursion to interpret $\ha[X,\|\cdot\|]$ we do not have to go through 
the model of strongly majorizable functionals and so do not need to assume 
any smallness condition on the types of $\underline{a}$ to pass to 
${\cal S}^{\omega,X}$)   
\[ {\cal S}^{\omega,X} \models \exists l,V,N \forall U,x\ \Big(\forall m\leq l 
\big (\psi_0(Um,m,\tup a) \vee \neg\psi_0(VxU,m,\tup a)\big)
\rightarrow \phi_0(x,N(U),\tup a)\Big).
\]
where `$\exists l,V,N$' is witnessed (uniformly in $\underline{a}$) 
by closed terms $t,s_V,s_N$ of 
$\ha[X,\|\cdot\|]$. 
The result when the terms $s_V,s_N$ are applied to 
$\tup a$, we conveniently name $V$ and $N$.\\
To show the learnability, let $U_{\tup v}$ (where $\tup v$ is a $t(\tup a)$-tuple) denote the function \[
U_{\tup v}(i):=\begin{cases}
v_i&\Tif\ i<t(\tup a),\\
0&\Telse,
\end{cases}\] set $B(\tup a):=t(\tup a)$ and define $L$ in $N$ and $V$ via a sequence of $t(\tup a)$-tuples $\tup v^{(\cdot)}$.
%Firstly, we set $\tup v^0 = 0,\ldots,0$, and $G(x,0,\tup a):=
More precisely to compute $L(\langle \underbrace {x_0,x_1,\ldots,x_i}_{\tup x:=}\rangle,\tup a)$ for some $i$ we need to
define the tuples $\tup v^{(0)},\ldots,\tup v^{(i)}$ as follows.
\begin{enumerate}
\item[$\tup v^0$] 
%If $\forall x \phi_0(x,0,\tup a)$ holds, then there is nothing to be done.  Otherwise, let $x:=x_0$ be the least counterexample. 
Set $\tup v^0:=0,\ldots,0$ and $c_1:=L(\langle x_0\rangle,\tup a):=\max(N(U_{\tup v^0}),x_0)$.
\item[$\tup v^1$] If $\forall x \phi_0(x,c_1,\tup a)$ holds, then there is nothing to be 
done\footnote{
Of course this is undecidable, however the conclusion discussed next is. In this sense if the conclusion is wrong, 
we can simply set $\tup v^1=\tup v^0$ and $L(\langle x_0,x_1\rangle,\tup a):=c_1$ (or even $0$ for all that it matters).
}. 
Otherwise, we have in particular
\[
\exists m\leq t(\tup a) \big (\neg\psi_0(U_{\tup v^0}m,m,\tup a) \wedge \psi_0(Vx_1(U_{\tup v^0}),m,\tup a)\big)
\]
so we can denote the least such an $m$ by $m_0$ and define $\tup v^1$ as $\tup v^0$ except that we set $v^1_{m_0}:=Vx_1(U_{\tup v^0})$.
Furthermore, we set 
$c_2=L(\langle x_0,x_1\rangle,\tup a):=\max( N(U_{\tup v^1}), x_1 )$. 
Note that we have \[ \neg\psi_0(U_{\tup v^0}m_0,m_0,\tup a) \wedge \psi_0(v^1_{m_0},m_0,\tup a). \tag{v0}\label{e:v0}\]
\item[$\tup v^2$] Now, if $\forall x \phi_0(x,c_2,\tup a)$ then we are finished. Otherwise, similarly as before we have 
\[
\exists m\leq t(\tup a) \big (\neg\psi_0(U_{\tup v^1}m,m,\tup a) \wedge \psi_0(Vx_2(U_{\tup v^1}),m,\tup a)\big)
\]
and we can denote the least such an $m$ by $m_1$ and define $\tup v^2$ as $\tup v^1$ except that we set $v^2_{m_1}:=Vx_2(U_{\tup v^1})$. As before this means that
\[ \neg\psi_0(U_{\tup v^1}m_1,m_1,\tup a) \wedge \psi_0(v^2_{m_1},m_1,\tup a), \tag{v1}\label{e:v1}\]
so in particular we obtain that $m_1\neq m_0$ by~\eqref{e:v0} as $U_{\tup v^1}m_1=v^1_{m_1}$. We set $c_3=L(\langle x_0,x_1,x_2\rangle,\tup a):=\max( N(U_{\tup v^2}), x)$ and continue.
\item[$\tup v^3$] Again, if $\forall x \phi_0(x,c_3,\tup a)$ then we are finished. Otherwise, as before, we have that
\[
\exists m\leq t(\tup a) \big (\neg\psi_0(U_{\tup v^2}m,m,\tup a) \wedge \psi_0(Vx_3(U_{\tup v^2}),m,\tup a)\big)
\]
and we can denote the least such an $m$ by $m_2$ and define $\tup v^3$ as $\tup v^2$ except that we set $v^3_{m_2}:=Vx_3(U_{\tup v^2})$. As before this means that
\[ \neg\psi_0(U_{\tup v^2}m_2,m_2,\tup a) \wedge \psi_0(v^3_{m_2},m_2,\tup a), \tag{v2}\label{e:v2}\]
so in particular we obtain that $m_2\neq m_1$ by~\eqref{e:v1}. Moreover, by~\eqref{e:v0} and~\eqref{e:v2} we have $m_2\neq m_0$, since from $m_1\neq m_0$ follows that $v^2_{m_0}=v^1_{m_0}$.
 We set $c_4=L(\langle x_0,x_1,x_2,x_3\rangle,\tup a):=\max (N(U_{\tup v^3}), x_3)$ and continue.\\
\item[$\tup v^{n+1}$] Finally, in general assume that for some $n$ we have that $\forall i< n\forall j<i\ m_i\neq m_j$
 and $\forall i\leq n+1 \neg \forall x \phi_0(x,c_i,\tup a)$.
Then we have also that
\[ 
\forall i<n \big (\neg\psi_0(U_{\tup v^i}m_i,m_i,\tup a) \wedge \psi_0(Vx_{i+2}(U_{\tup v^{i+1}}),m_i,\tup a)\big). \tag{vi}\label{e:vi}
\]
As usual we have in particular that
\[ 
\exists m\leq t(\tup a) \big (\neg\psi_0(U_{\tup v^n}m,m,\tup a) \wedge \psi_0(Vx_{n+1}(U_{\tup v^n}),m,\tup a)\big)
\]
and we can denote the least such an $m$ by $m_n$ and define $\tup v^{n+1}$ as $\tup v^n$ except that we set $v^{n+1}_{m_n}:=Vx_{n+1}(U_{\tup v^n})$. As before this means that
\[ \neg\psi_0(U_{\tup v^n}m_n,m_n,\tup a) \wedge \psi_0(v^{n+1}_{m_n},m_n,\tup a). \tag{vn}\label{e:vn}\]
%so in particular we obtain that $m_n\neq m_{n-1}$. \\
From $\forall 0<i<n\ ( m_0\neq m_i )$ it follows that $\forall0<i<n\ ( v^n_{m_0}=v^i_{m_0})$. Assume that $m_n = m_0$, then
\[U_{\tup v^n}m_n=v^n_{m_n}=v^n_{m_0}=v^1_{m_0}\]
and we obtain a contradiction as $\neg\psi_0(v^1_{m_0},m_0,\tup a)$ follows from~\eqref{e:vi} and $\psi_0(v^1_{m_0},m_0,\tup a)$ follows from~\eqref{e:vn}. This shows that 
$m_n \neq m_0$, similarly one shows that \[ \forall i<n\ (m_n \neq m_i).\]
As usual, we set $c_{n+2}=L(\langle x_0,\ldots,x_{n+1}\rangle,\tup a):=\max (N(U_{\tup v^{i+1}}),x_{n+1})$.
%  and therefore we obtain
%$m_n\neq m_0$ and in general that $\forall i<n m_n\neq m_i$ by the same argument.
\end{enumerate}
This leads to the following definition of $L$:
\begin{align*}
 L(x,\tup a):= 
 L(\langle \underbrace {x_0,x_1,\ldots,x_i}_{\tup x:=}\rangle,\tup a):= \max\{\max(N(U_{\tup v^{j}}), x_j)\ :\ j\leq i\}. 
\end{align*}
Note that since $N$ and $U$ are total, so is $L$. 
Moreover, if the values of $i$, $\tup x$, and $c$ satisfy the conditions from Proposition~\ref{p:allx}, then $L$ 
behaves as described above.\\
Finally, since there can be only $t(\tup a)$ many different $m_i$'s, it can happen at most $t(\tup a)$ many 
times that $\forall x \phi_0(x,c_i,\tup a)$ does not hold, where
$c_i$ is defined as in Proposition~\ref{p:allx} with $L$ as above. \\ The 
second claim follows from the fact that $t,s_V,s_v$ have majorants $t^*,
s^*_V,s^*_N$ 
given by closed terms of $\ha$ (see \cite{Kohlenbach(book)}) which then 
yield majorants $B^*,L^*$ of $B,L.$ Now apply  
Proposition \ref{p:majBL}.
\end{proof} \\[1mm] 
Using the representation of real numbers from 
\cite{Kohlenbach(book)}, each sequence of type $0\to 1$ can be viewed 
as a name of a sequence $(a_n)$ of reals. Now define 
$\tilde{a}_n:=\max_{\RR}\big(0,\min\nolimits_{i\le n}a_i\big).$ 
Let PCM$_{ar}(a_n)$ denote the statement that the monotone decreasing 
sequence $(\tilde{a}_n)$ in $[0,1]$ is Cauchy (see \cite{Kohlenbach(book)} 
for details) 
\[\mbox{PCM}_{ar}(a_n): \ \forall k\in\NN\,\underbrace{\exists n\in\NN\, 
\forall m\ge n\ (|\tilde{a}_m- \tilde{a}_n|<2^{-k})}_{\mbox{PCM}_{ar}((a_n),k)
:\equiv} \]
(if $(a_n)$ is already 
a decreasing sequence in $[0,1]$, then $(\tilde{a}_n)=(a_n)).$ 
The usual classical proof of PCM$(a_n)$ uses $\Sigma^0_2$-DNE, but it can be 
converted into a proof that only needs the weaker $\Sigma^0_1$-LEM (see 
Proposition \ref{p:limComp} below). 
In \cite{Toftdal}, an explicit such proof is constructed 
exhibiting a concrete sequence of instances of $\Sigma^0_1$-LEM sufficient 
for this. From this proof one can read off the following even more detailed 
fact:
\begin{prop} There is a primitive recursive functional (in the 
ordinary sense) $\Phi$ such that 
\[ \mbox{\rm HA}^{\omega}\vdash \forall a^{0\to 1}_{(\cdot)}, k^0\ 
\big( \forall m \le j(2^k-1,2^k) \,\Sigma^0_1\mbox{\rm -LEM}(\Phi(a_{(\cdot)},m)
\rightarrow 
\ \mbox{\rm PCM}_{ar}(a_{(\cdot)},k)\big). \] 
\end{prop} 
{\bf Proof.} The crucial step in Toftdal's proof in \cite{Toftdal} is 
to show by induction on $k$ that 
\[ \forall k\exists i\in \{ 1,\ldots,2^k\} \exists n \forall m \, 
\left( \frac{i-1}{2^k}\le \tilde{a}_{n+m}\le \frac{i}{2^k}\right), \]
where in the induction step $\Sigma^0_1$-LEM is used in the form 
(note that, based on our representation of real numbers, $<_{\RR}\in\Sigma^0_1$) 
\[\exists n\ \left( \tilde{a}_n <\frac{2i-1}{2^{k+1}}\right) \vee 
\neg \exists n\ \left( \tilde{a}_n <\frac{2i-1}{2^{k+1}}\right). \]
So to establish PCM$_{ar}(a_{(\cdot)},k)$ one needs all the instances 
\[\exists n\ \left( \tilde{a}_n <\frac{i}{2^{j}}\right) \vee 
\neg \exists n\ \left( \tilde{a}_n <\frac{i}{2^{j}}\right) \] 
for $i\le l-1$ and $l\le 2^k,$ i.e. the codes $j(i,l)$ of the instances 
used can be bounded by $t(k):=j(2^k-1,2^k).$  The construction of $\Phi$ is 
clear. 
 \hfill $\Box$ 

\mbox{ } 

While the usual classical proof of PCM$_{ar}$ only needs $\Sigma^0_1$-induction 
(but $\Sigma^0_2$-DNE), the above proof due to Toftdal needs an instance of 
the $\Sigma^0_2$-induction rule ($\Sigma^0_2$-IR) which, apparently, is the 
price to be paid for using only $\Sigma^0_1$-LEM (instead of 
$\Sigma^0_2$-DNE). Classically, $\Sigma^0_2$-IR is quite strong and proves 
(relative to PRA) the same $\Pi^0_3$-sentences as $\Pi^0_2$-IA 
(see e.g. \cite{Sieg}[Theorem 3.11]) and so, 
in particular, the totality of the Ackermann function. In our intuitionistic 
context, however, it is weak and the functional interpretation used 
(without negative translation!) in the 
proof of Theorem~\ref{t:bdLem} (and the corollary below) to extract $B,L$ solves 
$\Sigma^0_2$-IR using only ordinary primitive recursion in the form of 
$R_0.$ 
\\[2mm]
As a corollary we obtain that Theorem~\ref{t:bdLem} also holds with 
the original assumption being replaced by 
PCM$_{ar}(s(\underline{a},l),t(\underline{a})),$ where 
$(s(\underline{a},l)^{0\to 1})_l$ represents some sequence of reals defined by 
a closed term $s$ in $\underline{a}:$
\begin{cor}
Given that 
\[\ba{l} \mbox{\rm HA$^{\omega}[X,\|\cdot\|]+$AC$+$M$^{\omega}+
$IP}^{\omega}_{\forall}\vdash \\[1mm] \hspace*{2cm}  
\forall \underline{a}\,\exists k^0 ,l^0\ \big( 
\ \forall m \le l\ \mbox{\rm PCM}_{ar}(s(\underline{a},l),k)
 \rightarrow \exists n^0\forall x^0\,\varphi_0(x,n,\underline{a})\big), 
\ea \] 
where $s$ is a closed term and $\varphi_0$ as in Theorem \ref{t:bdLem}, then 
$\exists n^0\forall x\,\varphi_0(x,n,\underline{a})$ 
is (valid in ${\cal S}^{\omega,X}$) 
$B,L$-learnable (uniformly in $\underline{a}$) by functionals given by 
closed terms of the system $\ha[X,\|\cdot\|]$.\\
To $B,L$ one can construct majorants 
$B^*,L^*$ given by closed terms of $\ha$ such that 
if $\exists n\forall x \phi_0(x,n,\tup a)$ is monotone (see Definition~\ref{d:fmcMon}) then it is even learnable in $B^*,L^*$ uniformly in
majorants $\underline{a}^*$ of the parameters $\underline{a}$.
\end{cor}   
\begin{remark}
Of course, instead of sequences in $[0,1]$ one can also consider sequences 
in any compact interval $[-C,C],$ where then the functionals $B,L$ will 
additionally depend on $C.$ \\[1mm] Likewise, instead of decreasing 
sequences we may also have increasing ones or, if the Cauchy property 
is changed into the existence of an approximate infimum 
\[ \exists n\,\forall m (a_n\le a_m+2^{-k}), \] 
also arbitrary sequences in $[-C,C].$
\end{remark}

\begin{prop}\label{p:bg2meta}
Given any quantifier-free formula $\phi_0$ in the language of $\ha$ (or $\ha[X,\|\cdot\|]$), a rate of metastability (valid in ${\cal S}^{\omega,X}$) for (note that in order to talk about metastability, we need  one of the parameters, to have type 0 and we treat it separately) 
\[
\forall n^0 \exists m^0 \forall k^0\ \phi_0(n,m,k,\underline{a})\tag{$\phi$}\label{e:phi}
\]
is given by $\Omega(B^*,L^*,\underline{a}^*)$ 
(uniformly in majorants $\underline{a}^*$ of 
the parameters $\underline{a}$), where $\tilde g(c):=\max_{c'\leq c}(g(c'))+c$ and
\begin{align*} 
  \Omega&:=\lambda B^*,L^*,\underline{a}^*,g,n\ .\ C(L^*,g,n,
B^*(n,\underline{a}^*)),\\
C(i):=C(L^*,g,n,i,\underline{a}^*)&:=
\begin{cases}0,&\Tif\ i=0,\\ L^*(\langle\overbrace{ \tilde g(C(i-1)+1),\ldots,\tilde g(C(i-1)+1) }^{i\times}\rangle,n,\underline{a}^*),&\Telse,\end{cases}
\end{align*}
whenever $\exists m^0\forall k^0\,\varphi_0(n,m,k,\underline{a})$ 
is $B$, $L$-learnable 
(uniformly in $n$ and  $\underline{a}$) with $B^*,L^*$ majorizing 
$B,L$. \\ 
Note that $\Omega$ is defined using only recursion $R_0$ of type $0$ and 
hence is primitive recursive in the usual sense of Kleene.
\end{prop}
\begin{proof} We reason in ${\cal S}^{\omega,X}.$
Since $\phi_0$ is quantifier-free, there is a closed term $f$ with 
$f(n,m,k,\underline{a})=0\leftrightarrow \phi_0(n,m,k,\underline{a})$. 
Hence, we have that
\[ \forall n\ \exists i\leq B^*(n,\underline{a}^*)\ \forall k\ \ \big( 
f(n,c_i,k,\underline{a})=0 \big), \]
by the assumptions of the proposition, and we need to show that
\[ \forall g,n\ \exists m\leq \Omega(B^*,L^*,\underline{a}^*,g,n) 
\ \ \big( f(n,m,g(m),\underline{a})=0 \big). \]
Now, fix any $g$, $n$ and assume towards contradiction that 
$\Omega(B^*,L^*,\underline{a}^*,g,n)$ is not a rate of metastability, i.e. that
\[ \forall m\leq \Omega(B^*,L^*,\underline{a}^*,g,n)  \ \ \big( 
f(n,m,g(m),\underline{a})\neq0 \big). \tag{NE}\label{e:NE}\]
By induction on $i$ we obtain that
\[ \forall i\leq B^*(n,\underline{a}^*) \,\big(c_i\leq C(L^*,g,n,i,\underline{a}^*)
\big).\tag{IC}\label{e:IC} \]
The case $i=0$ is trivial as $c_0=C(L^*,g,n,0)=0$. Next, suppose for some $1\leq i\leq B^*(n,\underline{a}^*)$ the following holds
\[ \forall j<i\ \big(c_j\leq C(L^*,g,n,j,\underline{a}^*)\big). \tag{IH}\label{e:IH}\]
Denoting the smallest $k$ s.t. $f(n,m,k,\underline{a})\neq0$ by $x_m$ 
(if this does not exist, we have $c_i=c_{i-1}$ and are done), 
we obtain by~\eqref{e:IH} that\footnote{Here
we simply assume that our encoding is monotone in its components. If for some reason it was not, we could use a $L'$ which returns
the maximal value among all codes coordinatewise bounded by the elements of the encoded input of $L^*$.}
\[
   c_i\leq L^*(\langle x_{c_0},\ldots,x_{c_{i-1}}\rangle, n,\underline{a}^*)
   \leq L^*(\langle \tilde g({c_0}),\ldots,\tilde g({c_{i-1}})\rangle, n,\underline{a}^*)=C(L^*,g,n,i),
\]
since by~\eqref{e:NE} we have that $\forall i\leq B^*(n,\underline{a}^*)\ 
\big(m\leq C(L^*,g,n,i,\underline{a}^*)\rightarrow f(n,m,g(m),\underline{a})\neq0 \big)$ and 
so, in particular, that
$\forall i\leq B^*(n,\underline{a}^*)\ \big(m\leq C(L^*,g,n,i,\underline{a}^*)
\rightarrow x_m\leq g(m)\leq \tilde g(m) \big)$.\\
Finally, we can infer from~\eqref{e:IC} that
\[ \forall i\leq B^*(n,\underline{a}^*)\ 
\big(c_i\leq \Omega(B^*,L^*,\underline{a}^*,g,n)\big),\]
and therefore and by~\eqref{e:NE} also
\[ \forall i\leq B^*(n,\underline{a}^*)\ \neg\forall k^0\big( 
f(n,c_i,k,\underline{a})=0\big) \]
for all majorizable $\underline{a}$ contradicting the $B,L$-learnability 
(uniformly in $n$ and $\underline{a}$) of 
$\exists m\forall k\,\varphi_0(n,m,k,\underline{a})$ and the fact that $B^*$ 
majorizes $B$ (so that $B^*(n,\underline{a}^*)\ge B(n,\underline{a})).$

\end{proof}

\begin{rmk}\label{r:metastr}
Note that $\Omega$ has essentially the following form
\begin{align*}
&(L_{n,\tup a}\circ g)^{B^*(n,\tup a)}(0),\\
&L_{n,\tup a}:=\lambda x\ .\ L^*(x,n,\tup a^*).
\end{align*}
\end{rmk}


\section{Which Cauchy statements are effectively learnable and which are not.}


\begin{lemma}\label{l:Atrans+}
Friedmann's $A$-translation is sound also for $\ha + \LEM$.
\end{lemma}
\begin{proof}
It suffices to show that
\[\ha + \LEM\vdash (\LEM)^A.\]
This means we need to prove
\[ \forall y \big( \phi_0(\tup a,y) \vee A\big) \vee \exists y \big(  (\phi_0(\tup a,y) \vee A) \rightarrow A\big),\tag{1}\]
in $\ha + \LEM$. W.l.o.g assume $\phi_0$ is atomic. 
Consider the following instance of $\LEM$
\[\forall y \phi_0(\tup a,y)  \vee \exists y \neg\phi_0(\tup a,y).\]
Suppose that
\begin{enumerate}
\item $\forall y \phi_0(\tup a,y)$ holds. Then also $\forall y \big( \phi_0(\tup a,y) \vee A\big)$ holds and therefore also (1).
\item $\exists y \neg\phi_0(\tup a,y)$ holds. Then fix such a $y$. For this $y$ we get 
\[\big(  (\phi_0(\tup a,y) \vee A) \rightarrow A\big)\ \leftrightarrow (A\rightarrow A)\]
so $ \exists y \big(  (\phi_0(\tup a,y) \vee A) \rightarrow A\big)$ holds and therefore also (1).
\end{enumerate}
\end{proof}

%\begin{lemma}[Kohlenbach]\label{l:G3ACP}
%\[
%G_3A^\omega + \SiLm\IA \vdash \forall x^0\ \exists u^0 \forall\tilde x\leq x \big(  \forall y^0 \phi(\tilde x,y)\vee\exists\tilde u\leq u\neg\phi(\tilde x,\tilde u) \big)
%\]
%\end{lemma}
%\begin{proof}
%See \cite{Kohlenbach98} Lemma 3.18.
%\end{proof}

\begin{prop}\label{p:Atrans}
The theory $\ha + \LEM$ is closed under the $\Sigma^0_2\m\DNE$ rule.
\end{prop}
\begin{proof} 
Suppose \[\ha + \LEM \vdash \neg\neg \exists x\forall y\ \phi_0(\tup a,x,y),\]
where $\phi_0$ is quantifier free and contains only $\tup a$ as free variables. Moreover, w.l.o.g we assume that $\phi_0$ is atomic.  \\
Rewriting the negations in terms of ``$\rightarrow$'' and ``$\perp$'' we obtain that
\[\ha + \LEM \vdash \big(\ \exists x\forall y\ \phi_0(\tup a,x,y)\rightarrow\perp\big)\rightarrow \perp,\]
and using Friedman's A-translation (with Lemma~\ref{l:Atrans+}) that
\[\ha + \LEM \vdash \Big(\ \exists x\forall y\ \big(\phi_0(\tup a,x,y) \vee A\big)\rightarrow A\Big)\rightarrow A,\]
for any formula $A$ (not containing $x,y$ free). By setting \[A:\equiv\ \exists x'\forall y' \phi_0(\tup a,x',y'),\]
(we consider only this $A$ throughout the remainder of the proof) we obtain that $\ha + \LEM$ proves
\[\big(\ \exists x\forall y\ (\phi_0(\tup a,x,y) \vee \exists x'\forall y' \phi_0(\tup a,x',y'))\rightarrow \exists x'\forall y' \phi_0(\tup a,x',y')\big)\rightarrow \exists x'\forall y' \phi_0(\tup a,x',y').\tag{1}\]
Now the claim follows from
\[
\LEM\vdash \forall y\big(\phi_0(\tup a,x,y) \vee \exists x'\forall y' \phi_0(\tup a,x',y') \big) \rightarrow \big(\forall y\ \phi_0(\tup a,x,y) \vee \exists x'\forall y' \phi_0(\tup a,x',y')\big),
\tag{2}
\]
since using (2) the statement (1) is equivalent to
\[  \big((\exists x \forall y \phi_0(\tup a,x,y) \vee \exists x'\forall y' \phi_0(\tup a,x',y'))\rightarrow \exists x'\forall y' \phi_0(\tup a,x',y')\big)\rightarrow \exists x'\forall y' \phi_0(\tup a,x',y'),\]
which is equivalent to $\exists x\forall y \phi_0(\tup a,x,y)$.\\
To show (2) consider the following instance of $\LEM$
\[
\forall y\ \phi_0(\tup a,x,y) \vee \exists y\ \neg\phi_0(\tup a,x,y).
\]
Now suppose that
\begin{enumerate}
\item $\forall y\ \phi_0(\tup a,x,y)$ holds, then (2) is trivially true.
\item $\exists y\ \neg \phi_0(\tup a,x,y)$ holds, then for such a $y$ we have
\[
\big(\phi_0(\tup a,x,y) \vee \exists x'\forall y' \phi_0(\tup a,x',y') \big) \rightarrow \exists x'\forall y' \phi_0(\tup a,x',y')
\]
and so certainly we have also that
\[
\forall y\big(\phi_0(\tup a,x,y) \vee \exists x'\forall y' \phi_0(\tup a,x',y') \big) \rightarrow \exists x'\forall y' \phi_0(\tup a,x',y').
\]
Finally $\big(\forall y\ \phi_0(\tup a,x,y) \vee \exists x'\forall y' \phi_0(\tup a,x',y')\big)$ follows from $\exists x'\forall y' \phi_0(\tup a,x',y')$ so (2) holds as well.
\end{enumerate}
%Finally the use of collection can be omitted in the (classical!) system $G_3A^\omega + \SiLm\IA$ due to Lemma~\ref{l:G3ACP}.
\end{proof}

It is known, that a Cauchy rate is limit computable for every (provable) 
Cauchy sequence. While in general fmc is stronger than being limit computable, in the specific case of Cauchy property we have the following rather surprising circumstance:
\begin{prop}\label{p:limComp}
If a sequence of real numbers $(a_n)$ (or in some $\pa$-definable Polish 
space) defined by a closed term of 
$\pa$, can be proved to be Cauchy in $\pa$, then the proof can be carried 
out already in $\ha+\LEM$. Similarly for $\pa[X,\|\cdot\|]$ and sequences in 
$X.$
\end{prop}
\begin{proof}
Consider a sequence $x_{(\cdot)}$ and suppose
\[ \pa\vdash \forall k\exists n\forall i,j>n\ \big(| x_i - x_j| \leq 2^{-k}\big). \]
Then by the Kuroda negative translation we obtain that
\[ \ha\vdash \forall k\neg\neg\exists n\forall i,j>n\ \big(| 
x_i - x_j| \leq 2^{-k}\big). \]
By Proposition~\ref{p:Atrans} this implies that
\[ \ha+\LEM\vdash \forall k\exists n\forall i,j>n\ \big(| x_i - x_j| \leq 2^{-k}\big) \] (recall that $\le_{\RR}\in\Pi^0_1$).
%which implies the claim by~\ref{c:lem2fmc}.
\end{proof}



\begin{prop}[Implications between different bounding information 
for Cauchy statements] \label{prop.hierarchy}
Let $(x_n)$ be a Cauchy sequence in a metric space $(X,d).$ 
\begin{enumerate}
\item A rate of convergence is a bound on the number of fluctuations.
\item A bound for the number of fluctuations is a bound $B$ 
on the number of mind changes to learn a rate of convergence (with a very 
simple learning function $L$, which is basically the successor function).
\item 
Primitive recursively (in the ordinary sense of Kleene) in majorants 
$B^*,L^*$ of functionals $B,L$ such that the Cauchy rate is $(B,L)$-learnable 
one can obtain a rate of metastability. 
\end{enumerate}
\end{prop}
\begin{proof}
Consider a Cauchy sequence $x_{(\cdot)}$.
\begin{enumerate}
\item Let $b$ be a rate of convergence, i.e.
\[ \forall k \forall n,m\geq b(k)\ \big( d(x_n,x_m)\leq 2^{-k}\big). \]
Then $b(k)$ is also a bound on the number of $2^{-k}$ fluctuations, since any fluctuation has to occur before $b(k)$ (i.e. that one of the indexes of the 
fluctuation has to be smaller than $b(k)$) and there can be at most $b(k)$ many fluctuations indexed within $[0;b(k)]$.
\item Let $b$ be a bound on the number of $2^{-k}$ fluctuations, i.e.
\[ \forall k \forall n>b(k) \forall i,j \neg\Fluc_{2^{-k}}(n,i,j). \]
Then $b(k)$ is also a bound on the number of mind changes to learn a rate of convergence, since for 
$L(n):=n+1$ we have that
\[ \forall k\ \exists l\leq b(k)\ \forall n,m>c_l\ \ \big(  d(x_n,x_m)\leq 2^{-k}\big). \tag{BE}\label{e:be}\]
%
\begin{itemize}
\item{ }
Formally, $L(l,x_{(\cdot)},k):=l$, and
\[
\phi_0(\langle n,m\rangle,c_i,x_{(\cdot)},k):\equiv\ \big((n>c_i \wedge m>c_i) \rightarrow d(x_n,x_m)\leq 2^{-k}\big),
\]
where $\langle n,m\rangle$ is any surjective pairing function satisfying
\[
\forall n,m \,(n,m\le \langle n,m\rangle),\] e.g. the Cantor pairing function.
\end{itemize}
%
The statement~\eqref{e:be} can be inferred from the fact that each mind change ($c_i$) corresponds to a (different) fluctuation (as it is based on a counterexample for $d(x_n,x_m)\leq 2^{-k}$, whose both indexes are greater than the last $c_i$).
\item Follows directly from Proposition~\ref{p:bg2meta}.
\end{enumerate}
\end{proof}


%Since we didn't assume the bound on mind changes $B$ to be computable, we showed that even limit computable rate of convergence implies the existence of a rate of metastability which is primitive recursive in $B$. To capture the somewhat more general connection between metastability (not only for Cauchy statements) and mind changes we give the following proposition.
%
%\begin{prop}[fmc and metastability]
%In general, if a universal arithmetical predicate $\forall x^0 \phi_0(n,c_i,\tup a)$ can be learned in the parameters $\tup a$ with finitely many mind changes,
%then the metastable formulation has a computable solution. Meaning that there is a computable functional $M$, s.t.
%\[ \forall g^1 \forall x\leq g( M(g) )\ \phi_0(x, M(g), \tup a).\tag{A}\label{e:A}\]
%\end{prop}
%\begin{proof}
%To prove the contrapositive, suppose not \eqref{e:A} which means that (for any computable $M$ so specifically also) for any $M(n):=m$ we have an $x$ s.t. \[
%\neg \phi_0(x, m,\tup a)
%\]
%obviously $\forall x^0 \phi_0(n, c_i,\tup a)$ is then refuted by $M(n):=c_i$ for any $c_i$. So there cannot be any bound $B(\tup a)$, especially not a computable one.
%\end{proof}

\section{Separating counterexamples}
In this section we show that the hierarchy between the three different 
quantitative notations for Cauchy sequences in Proposition 
\ref{prop.hierarchy} is strict. That an effective bound on the number of fluctuations does not imply an 
effective rate of convergence, follows already from the existence of Specker sequences. We can also use the following
very simple example with a $2^{-k}$-fluctuation bound $k$ and no effective rate of conrgence, since such a rate would decide the (partial) holding problem:\\ 
\begin{dfn}[$\alpha_{(\cdot)}$] 
We fix a surjective pairing and set
\[
\alpha_{\langle k,n\rangle}:=
\begin{cases}
2^{-k}, &\Tif T(k,0,n),\\
0, &\Telse.
\end{cases}
\]
\end{dfn}
In \cite{Avigad/Rute} Avigad and Rute discuss why there are no reverse transformations in general scenarios (omitting learnability, as they don't consider this form of finitisation). It is natural to assume that an effective rate of metastability (even primitive recursive) does not imply the existence of an effective bound on fluctuations, but it seems that so far there is no formal proof of this fact.
%follows from Proposition~\ref{p:limComp} (which relies on the Conjecture!). 
We give a formal counterexample, i.e. a primitive recursive sequence $\beta_{(\cdot)}$ with an effectively (even primitive recursively) learnable Cauchy rate (so in particular with a primitive recursive rate of metastability), which has no computable bound on fluctuations (this example is not captured by the rough sketch of Avigad and Rute as here the number of the oscillations is determined by the length of the computation, not by the index of the machine as suggested in \cite{Avigad/Rute}). % to the reverse of \eqref{e:fmc} implies \eqref{e:meta} as follows:
Furthermore, we also give an example of a primitive recursive (in the ordinary 
sense) sequence $\gamma_{(\cdot)}$ in $\RR$ which (provably in PA) is Cauchy (and so 
has a rate of metastability definable in $\pa$) 
which does not have an effectively learnable Cauchy rate. 

\subsection{A sequence with an effectively learnable Cauchy rate with no computable bound on fluctuations}

\begin{dfn}[$\beta_{(\cdot)}$] 
We fix a surjective encoding of triples which is monotone in the third component and set
\[
\beta_{\langle k,n,l\rangle}:=
\begin{cases}
2^{-k}, &\Tif T(k,0,n) \wedge l\leq n \wedge l\text{ is even},\\
0, &\Telse.
\end{cases}
\]
\end{dfn}

In the next propositions we will show that the sequence $\beta_{(\cdot)}$
\begin{itemize}
\item is Cauchy (i.e. converges to zero) -- Proposition~\ref{p:alphaIsCauchy},
\item its Cauchy rate is effectively learnable -- Proposition~\ref{p:alphaIsLearnable},
\item there is no computable (in $\epsilon$ and $\beta$) bound on the number of $\epsilon$-fluctuations -- Proposition~\ref{p:alphaHasNoFlucBd}.
\end{itemize}

\begin{prop}\label{p:alphaIsCauchy}
The sequence $\beta_{(\cdot)}$ is Cauchy, provably in 
{\rm $\ha+\Sigma^0_1$-LEM$^-$}. More precisely we show that
\[ 
\ha
\vdash 
\forall k \Big( 
\forall m\leq k \exists u \forall v\ \big(T(m,0,u) \vee \neg T(m,0,v)\big)\rightarrow \exists n \forall x\geq n\big(\beta_x\leq2^{-k}\big)\Big).
 \]
\end{prop}
\begin{proof}
Consider the terminating computations on input $0$ of the Turing machines encoded by $0,\ldots,k$. Then for every $k$ there is an $n$ corresponding to the
code of the longest such computation. W.l.o.g. we can assume that $n\geq k$ (otherwise set $n:=k$). This means we have that
\begin{align}
n\geq k\ \wedge\ \forall n'\forall k'\leq k\ \big( T(k',0,n')\rightarrow n'\leq n\big).\label{e:ac-n}
\end{align}
Now, set \[ c(k):=\max\{ \langle k',n',l' \rangle\ :\ n'\leq n,\ k'\leq k,\ l'\leq n'\}.\]
Then $c$ is even a rate of convergence, since
\[
\langle k',n',l'\rangle\ >\ c(k)\ \rightarrow\ k'>k \vee n'>n\ 
\rightarrow\ \beta_{\langle k',n',l'\rangle}=0 \vee k'>k\ \rightarrow\ \beta_{\langle k',n',l'\rangle} < 2^{-k}.
\]
These arguments are constructive, except for the existence of the longest computation $n$.
This assumption is a consequence of $\LEM^-$ and $\Pi^0_1\m\CP^-$.
Consider the following $k$ instances of $\LEM$:
%\[ \exists n_0 T(0,0,n_0) \vee \forall m \neg T(0,0,m),\ \ldots,\ \exists n_k T(k,0,n_k) \vee \forall m \neg T(k,0,m). \]
\[ \forall j\leq k\ \big( \exists n T(j,0,n) \vee \forall m \neg T(j,0,m)\big). \]
So in particular we obtain from $\LEM$ that
\[ \forall j\leq k \exists n_j \big( T(j,0,n_j) \vee \forall m \neg T(j,0,m)\big), \]
which implies
\begin{align*}
\exists n\forall j\leq k ( \exists n'\leq n T(j,0,n') \vee \forall m \neg T(j,0,m) ),
\end{align*}
 (consider $n=\max \{n_j : j\leq k\}$).\\
This shows that the convergence is provable in $\ha$+$\LEM^-$ and the convergence up to an error $2^{-k}$ in $\ha$ using
only $k$ instances of $\LEM^-$.
\end{proof}



\begin{lemma}[Kohlenbach]\label{l:G3ACP}
For a quantifier-free formula $\phi_0$ with parameters only of type $0$, we have that
\[
\mbox{\rm G$_3$A}^\omega + 
\SiLm\IA^- \vdash 
\forall x^0\ \exists u^0 
\forall\tilde x\leq x \big(  
\forall y^0 \phi_0(\tilde x,y)
\vee\exists\tilde u\leq u\neg\phi_0(\tilde x,\tilde u) \big)
\]
\end{lemma}
\begin{proof}
See \cite{Kohlenbach(book)} Lemma 3.18.
\end{proof}

\begin{rmk}
In general, it seems that considering extraction of computational content from proofs, often some amount of classical logic can be bought on the cost of more recursion.\\
If one is interested (only) in a classical proof, we obtain due to Lemma~\ref{l:G3ACP}
(simply consider $A(x,y):\equiv \neg T(x,0,y)$) a proof without the use of $\PiLm\CP$, which can be formalized in G$_3$A$^{\omega}+\SiLm\IA^-$.
\end{rmk}


\begin{prop}\label{p:alphaIsLearnable}
The rate of convergence is effectively learnable in $k$, i.e.
there are total (elementary) 
recursive functions $B$ and $L$, s.t. for any $\beta_{(\cdot)}$, and any $k$ we have that
\[ \forall k\ \exists n\leq B(k)\ \forall m > c_n\quad \big(\beta_m\leq 2^{-k}\big),\]
where $c_{(\cdot)}$ is defined as in Definition~\ref{d:fmcMon} with
\[
\phi_0(x,n,\tup a):\equiv\ x\geq n\rightarrow \beta_x\leq 2^{-k}.
\]
%\[ \forall k,m,n \Big( \big( n>c_m \wedge \beta_n > 2^{-k} \big) \rightarrow  G(n,c_m)=c_{m+1} \Big). \]
\end{prop}
\begin{proof}
Obviously, this follows already from Proposition~\ref{p:alphaIsCauchy}. We \todo{can} show that the rate is $(B,L)$-learnable with the following $B$ and $L$:
\begin{align*}
B(k)&:=k+1  \\
L(\langle \tup x \rangle,k)&:= \max\{ \langle k',n',l' \rangle\ :\ k',n',l'\leq \max(\tup x) \}.
\end{align*}
\end{proof}


\begin{cor} \label{metastability-alpha}
$\beta_{(\cdot)}$ has a primitive recursive (in the ordinary sense 
of Kleene) rate of metastability.
\end{cor}
\begin{proof} One can apply Proposition \ref{p:bg2meta} to convert 
the bounds $(B,L)$ from Proposition \ref{p:alphaIsLearnable} (which are 
trivially self-majorizing using standard monotonicity properties of the 
triple coding) into a primitive 
recursive rate of metastability. Alternatively, one can use that by 
Lemma \ref{l:G3ACP} the Cauchy property of $\beta_{(\cdot)}$ is provably in 
G$_3$A$^{\omega}+\Sigma^0_1$-IA$^-$ and so a fortiorily in 
$\widehat{\rm PA}^{\omega}\res+$QF-AC (see \cite{Kohlenbach(book)}, Prop.3.31).
Then proposition 10.54 in \cite{Kohlenbach(book)} implies the extractability 
of a primitive recursive rate of metastability (the latter being essentially 
the rate of metastability for the statement in Lemma \ref{l:G3ACP} 
which is computed in \cite{Kohlenbach(book)}[Prop.3.19]). 
\end{proof} 
\begin{lemma}[Termination causes at least $n$ fluctuations]\label{l:2n}
Suppose the $k^\text{th}$-machine terminates on $0$ with computation encoded by $n$ (i.e. $T(k,0,n)$ holds). Then the sequence $\beta_{(\cdot)}$ contains at 
least $2n$ many $2^{-k}$\nbd fluctuations.
\end{lemma}
\begin{proof}
Consider the tuples of indexes $\tup i$, $\tup j$, 
s.t. $i_l:=\langle k,n,l\rangle$, $j_l:=\langle k,n,l+1\rangle$ and $l+1\leq n$. Then we have by definition of $\beta_{(\cdot)}$ (in particular also by the monotonicity of the encoding in $l$) that
$
\Fluc_{\beta_{(\cdot)},2^{-k}}(n,\langle \tup i \rangle, \langle \tup j\rangle).
$
\end{proof}

\begin{prop}\label{p:alphaHasNoFlucBd}
There is no computable bound on the fluctuations of $\beta_{(\cdot)}$.
\end{prop}
\begin{proof}
Suppose $b_k$ is a bound on the number of fluctuations by $2^{-k}$, 
then $b_k$ can be used to effectively compute whether the $k^{\text{th}}$ Turing machine terminates on input $0$ as follows.\\
Let the machine run until the code of the computation reaches $b_k$ (or until it stops). If it terminated, we are done.\\
Now suppose it terminates with a computation encoded by some $n>b_k$. Then $\beta_{(\cdot)}$ would have at least $n$ many $2^{-k}$\nbd fluctuations by Lemma~\ref{l:2n}, which is a contradiction.\\
Therefore if the machine does not terminate with a code of computation at most $b_k$ it does not terminate at all.
\end{proof}



\subsection{Metastability of Cauchy sequences does not imply effective learnability}

In next propositions we will develop a sequence $\gamma_{(\cdot)}$ (defined in Corollary~\ref{c:gammaee} using Definition~\ref{d:gammaf}) that
\begin{itemize}
\item is Cauchy (i.e. converges to zero) -- by Proposition~\ref{p:gammaf},
\item has a primitive recursive rate of metastability -- by Proposition~\ref{p:gammaf},
\item has no effectively learnable Cauchy rate -- by Proposition~\ref{p:nonLearnablePhi} .
\end{itemize}

We use the upper index as a name extension (like $\tup k^K$, meaning a tuple $\tup k$ corresponding to a particular $K$) and as iteration of functions (like $f^n(x)$, meaning we iterate the function $f$ $n$-many times with the starting point $x$). When unclear, we use the notation $(f)^n$ to make explicit, that we mean the iteration.


\begin{dfn}
For any function $f$ we define $\hat f(k,n)=0$ if $f(k,n)=0\wedge\forall m<n\ (f(k,m)\neq 0)$ and $\hat f(k,n)=1$ otherwise.
\end{dfn}

\begin{prop}\label{p:nonLearnablePhi} The following statement
\[
\phi:\equiv\quad\forall f^1\leq_1 1 \forall x^0 \exists p  \forall z \phi_0 (\hat f,x,p,z)
\]
where, interpreting $p$ as a surjective code of some pair $y,u$,
\begin{align*}
\phi_0(f,x,p,z):\equiv\quad &\forall \tilde x \leq x \exists \tilde y\leq y \forall \tilde z\leq z 
\big(f(\tilde x,\tilde y)=0\vee f(\tilde x,\tilde z)\neq0\big) 
\wedge \\
&\forall \tilde y \leq y_{f,x} \exists \tilde u\leq u \forall \tilde z\leq z 
\big(f(\tilde x,\tilde u)=0\vee f(\tilde y,\tilde z)\neq0\big),
\end{align*}
with
\[
y_{f,x}:=\begin{cases}
\max\big\{y'\leq y\ :\ y'=\min\{y''\leq y\ :\ f(x',y'')=0\ \wedge x'\leq x \}\big\},&\text{if such $y'$ exists}\\
x,&\Telse.
\end{cases}
\]
is not effectively learnable.\end{prop}
\begin{proof}
We prove the claim in three steps.
\paragraph{Step 1} We will show that (informally speaking, formally we cannot express function iteration and the conclusion would have to use $\psi_0$ and $\psi^y_0$ defined below)
\[
G_3A^\omega\vdash \forall g \forall x\ \big(\exists p\forall z\phi_0(f_g,x,p,z)\rightarrow \exists y'=g^{g^{x}(0)}(0)\big),
\tag{GA}\label{e:GA}
\]
for \[
f_g(b,d):=\begin{cases}
0,&\Tif \ \lh(d) = b+1 \wedge d_0=0\wedge \forall i< b\ (d_{i+1}=g(d_i),\\
1,&\Telse.
\end{cases}
\]
Note that we have $f_g=_1\widehat{f_g}$. To show~\eqref{e:GA} fix arbitrary $g^1$ and $x^0$.
From $\phi$ we get in particular $\exists p\forall z\phi_0(f_g,x,p,z)$, let us fix such a $p=\langle y,u\rangle$ to obtain:
\begin{align}
&\forall z\forall\tilde x \leq x \exists \tilde y\leq y \forall \tilde z\leq z 
\big(f_g(\tilde x,\tilde y)=0\vee f_g(\tilde x,\tilde z)\neq0\big) \label{e:A}
\wedge \\
&\forall z\forall\tilde y \leq y_{f_g,x} \exists \tilde u\leq u \forall \tilde z\leq z 
\big(f_g(\tilde x,\tilde u)=0\vee f_g(\tilde y,\tilde z)\neq0\big). \label {e:B}
\end{align}
Now, set 
\[
\psi_0(x',y'):\equiv \lh(y')=x'+1 \wedge y'_0=0 \wedge \forall i< x'\ \big( y'_{i+1}=g(y'_i) \big),
\]
(informally $\exists y'\leq y\psi_0(x',y')$ means that $\exists y'=g^{x'}(0)$) to show by bounded quantifier-free induction that
%\[\forall x'\leq x \exists y'\leq y\ \psi_0(x',y').\tag{IC1}\label{e:IC1}\]\\
\be[e:IC1]
\forall x'\leq x \exists y'\leq y\ \psi_0(x',y').
\ee
The case $\psi_0(0)$ is trivially satisfied by $y'=\langle 0 \rangle$. So suppose for some $x'<x$ we have
$\exists y'\leq y\psi_0(x',y')$, then we can set 
\[
z:=\langle y'_0,y'_1,\ldots,g(y'_{x'})\rangle
\] to get $f_g(x'+1,z)=0$,
which by~\eqref{e:A} implies $\exists \tilde y\leq y f_g(x'+1, \tilde y)=0$ and so in particular (for $y'$ the smallest such $\tilde y$)
\[
\exists y'\leq y\ \psi_0(x'+1,y'),
\]
which concludes the proof of~\eqref{e:IC1}. Since \[ \psi_0(x',y')\ \rightarrow\ f_g(x',y')=0,\] 
and for $y'\leq y$ (and $x'\leq x$),
\[ \psi_0(x',y')\ \rightarrow\ y'\leq y_{f,x},\] 
we have even that (note also that if $\psi_0(x',y')$ the number $y'$ is unique for any $x'$)\\
\be[e:IC1+]
\forall x'\leq x \exists y'\leq y_{f_g,x}\ \psi_0(x',y').
\ee

%Next, set \[\psi'_0(x',y):=\exists u'\leq u \ \big(u'=g^{g^{x'}(0)}(0)\big),\]
Now, let $y^*$ denote a $y'$ which satisfies~\eqref{e:IC1+} for $x'=x$ and set \[
\psi^y_0(x',u'):=
\exists y'\leq y_{f_g,x} \Big( \psi_0(x',y') \wedge \lh(u')=y'_{x'}+1 \wedge u'_0=0 \wedge \forall i< y'_{x'}\ \big( u'_i+1=g(u'_i) \big)\Big),
\]
to show that \be[e:IC2]\forall x'\leq y^* \exists u'\leq u \ \psi^y_0(x',u').\ee
The case $\psi^y_0(0)$ is again trivially satisfied by $u'=\langle 0 \rangle$ (using~\eqref{e:IC1+} for $x'=0$).
So suppose for some $x'< y_{f_g,x}$ that
$\exists u'\leq u \psi^y_0(x',u')$. Then we can set
\[z:=\langle u'_0,u'_1,\ldots,g(u'_{x'})\] to get $f_g(x'+1,z)=0$,
which by~\eqref{e:B} implies 
\[\exists \tilde u\leq u f_g(x'+1, \tilde u)=0\]
and so, using~\eqref{e:IC1+}, we obtain (for $u'=\tilde u$)
\[
\exists u'\leq u\ \psi^y_0(x'+1,u'),
\]
which concludes the proof of~\eqref{e:IC2} and therefore also the proof of~\eqref{e:GA}.\\
\paragraph{Step 2} We investigate the terms witnessing the implication~\eqref{e:GA}. By prenexiation we obtain
\[
G_3A^\omega\vdash \forall g,x,p\exists y',z\ \big(\phi_0(f_g,x,p,z)\rightarrow y'=g^{g^{x}(0)}(0)\big),
\]
and, therefore, by program extraction theorems (see Corollary 3.1.3. in~\cite{Kohlenbach(lowrate)}), we get closed terms $s$ and $t$ in $G_3A^\omega$, s.t.
\be[e:st]
\forall g,x,p\ \big(\phi_0(f_g,x,p,sgxp)\rightarrow tgxp=g^{g^{x}(0)}(0)\big).
\ee
\paragraph{Step 3} Finally, we show that with sufficiently large (in the sense of growth) $g$, this contradicts the effective learnability of $\phi$. Suppose namely that $\phi$ was learnable,
then by Proposition~\ref{p:bg2meta} and the fact that $f_g$ is trivially majorizable (by $\lambda n^0.1^0$) we get in particular that 
\[
\exists p\leq \Omega(B^*,L^*,h,x)\ \phi_0(f_g,x,p, zgxp),
\]
for any $g$ and $x$, by setting \[h^1:=\lambda v\ .\ s gxv.\]
So, we obtain together with~\eqref{e:st} that
\[
\forall g,x \exists p\leq \Omega(B^*,L^*,h,x)\ \big( tgxp=g^{g^{x}(0)}(0)\big).
\]
Since $s$ and $t$ are closed terms of $G_3A^\omega$, so is $\lambda v\ .\ s gxv$, by normalisation arguments
(see Corollary 2.2.24 and Remark 2.2.25 in~\cite{Kohlenbach(lowrate)}) we know that there is 
a constant $D'$, s.t. $t^*:=\lambda g,x,p.g^{D'}(\Omega(B^*,L^*,g^{D'}(x),x))$ (note that it is safe to assume 
$x+p<2p<2\Omega(B^*,L^*,h,x)<g(\Omega(B^*,L^*,h,x))$) majorizes $t$. By the definition of $\Omega$ (see also Remark~\ref{r:metastr}) this means that there is a constant $D$, s.t. 
\[ \forall x^0\ (L^*(x)\circ g)^{B^*(x)D}\geq g^{g^{x}(0)}(0),\]
which is not possible for sufficiently fast growing $g$.
\end{proof}

\begin{cor}\label{c:anyStrongerPhi}
Let $\phi_0$ be as in the previous Proposition. If for a quantifier free formula $\psi_0$ (with no hidden parameters)
\[
G_3A^\omega+\AC\m\QF\vdash \forall f\leq 1 \forall x^0\big( \exists y^0\forall z^0 \psi_0(\xi(f),\chi(x),y,z)
 \rightarrow \exists p^0\forall z^0 \phi_0(\hat f,x,p,z) \big),
 %forall x^0 \exists p  \forall z \phi_0 (f,x,p,z)\big),
\]
where $\xi$ and $\chi$ are closed terms of $G_3A^\omega$,
then $\forall f\leq 1\forall x^0\exists y^0\forall z^0 \psi_0(f,x,y,z)$ is also not effectively learnable.
\end{cor}
\begin{proof}
This follows analogously from~\cite{Kohlenbach(lowrate)} (Corollary 3.1.3.) and our Proposition~\ref{p:bg2meta} as in the proof
of Proposition~\ref{p:nonLearnablePhi}.
\end{proof}

\begin{rmk}
We can prove in $G_3A^\omega+\SiLm\CP$ (which is included in $G_3A^\omega+\AC\m\QF$) that $\phi$ in Proposition~\ref{p:bg2meta} is actually equivalent to its monotone version, 
\[
\tilde\phi\equiv \forall f^1\leq 1\forall x^0\exists q\forall z \exists p\leq q\forall \tilde z\leq z \phi_0(\hat f,x,p,z).
\]
Since this equivalence holds also pointwise, we can use this Corollary to infer that there is actually a monotone formula, which is not effectively learnable.
\end{rmk}

\begin{dfn}\label{d:gammaf}
\[
\gamma_{\langle k,n,i,m\rangle}(f):=\begin{cases}
2^{-k},&\Tif\ \hat f(k,n)=0\wedge i\leq n\wedge \hat f(i,m)=0,\\
0,&\Telse,\end{cases}
\]
%where $\hat f(k,n)=0$ if $f(k,n)=0\wedge\forall m<n\ (f(k,m)\neq 0)$ and $\hat f(k,n)=1$ otherwise.
\end{dfn}

\begin{prop}\label{p:gammaf}
The formula stating the existence of a Cauchy point for any $f$ 
\[
\psi:=\forall f^1\leq_1 1,x^0\exists z \forall k\geq z\ \big( \gamma(f)_z<2^{-x} \big).
\]
though being true, is not effectively learnable. Moreover, for any $f\leq_1 1$ there is
a prim. rec. rate of metastability for $\gamma(f)\seq$, which is uniform in $f$.
\end{prop}
\begin{proof}
The existence of a metastability rate follows from the fact
that $\gamma(f)\seq$ is Cauchy for any $f\leq_1 1$. Moreover, since the
proof can be formalized in $G_3A^\omega+\SiLm\IA$ there is a primitive recursive rate and since $f$ is trivially majorizable it is also
clear that there is even a primitive recursive rate which is uniform in $f$. \todo{Below, we actually give such a rate explicitly.}\\
Given that, due to Corollary~\ref{c:anyStrongerPhi} it now suffices to show that
\[ G_3A^\omega+\AC\m\QF\vdash 
\forall f\leq 1 \forall x^0\big( \exists z \forall k\geq z\ \big( \gamma(f)_z<2^{-x} \big)
 \rightarrow \exists p\forall z' \phi_0(\hat f,x,p,z') \big).
 \]
To prove $\phi$ fix arbitrary $f^1$ and $x^0$ and suppose that
\[\exists z\forall k\geq z \big( \gamma(f)_z<2^{-x} \big).\]
Moreover, assume towards contradiction
\be[e:CD]
\exists \tilde x\leq x \exists a\geq\max(z,x) \hat f(\tilde x,a)=0. 
\ee
Since $\hat f(\tilde x,a)=0$ is by definition $f(\tilde x,a)=0 \wedge \forall \tilde a < a\ f(\tilde x,\tilde a)\neq 0$, 
this means that $k:=\langle \tilde x, a, \tilde x, a\rangle > z$ and $\gamma(f)_k=2^{-\tilde x}$, which is a contradiction.\\
From not~\eqref{e:CD}, we can conclude that
\[
\forall \tilde x\leq x\big( \exists\tilde y<\max(x,z) \hat f(\tilde x,\tilde y)=0\ \vee\ \forall a \hat f(\tilde x,a)\neq 0\big),
\] 
which is equivalent to
\be[e:A1]
\forall \tilde x\leq x \exists\tilde y<\max(x,z) \forall a\ \big(  \hat f(\tilde x,\tilde y)=0\ \vee\  \hat f(\tilde x,a)\neq 0\big).
\ee 
Next, set $y:=\max(x,z)$ and assume towards contradiction that
\be[e:CD2]
 \exists \tilde y\leq y_{\hat f, x} \exists a\geq z \hat f(\tilde y,a)=0. 
Recall that 
\ee
\[
y_{\hat f, x}:=\begin{cases}
\max\big\{y'\leq y\ :\ y'=\min\{y''\leq y\ :\ \hat f(x',y'')=0\ \wedge x'\leq x \}\big\},&\text{if such $y'$ exists}\\
x,&\Telse.
\end{cases}
\]
Note that if $y_{\hat f, x}=x$, then $\phi$ follows already from~\eqref{e:CD}. Otherwise, denote the smallest $x'\leq x$ for which
$\hat f (x',y_{\hat f, x})=0$ by $\tilde x$. Then $k:=\langle \tilde x, y_{\hat f, x}, \tilde y, a\rangle > z$ and $\gamma(f)_k=2^{-\tilde x}$, which is a contradiction.\\
Finally, $\exists p\forall z' \phi_0(\hat f,x,p,z')$ follows from not~\eqref{e:CD2} and~\eqref{e:A1} (with $y:=\max(x,z)$, $p:=\langle y, z\rangle$).
%\[
%\forall \tilde y\leq y_\big( \exists\tilde y<\max(x,z) \hat f(\tilde x,\tilde y)=0\ \vee\ \forall a \hat f(\tilde x,a)\neq 0\big),
%\] 
\\
\end{proof}

\begin{cor}\label{c:gammae}
Given a G\"odel numbering $\theta$ of total recursive functions, 
the formula stating the existence of a Cauchy point of $\gamma(\theta_e)\seq$ for any G\"odel number $e$ of a total recursive function 
\[
\psi:=\forall e^0,x^0\exists z \forall k\geq z\ \big( \gamma(\theta_e)_k<2^{-x} \big).
\]
is not effectively learnable.
\end{cor}

\begin{proof}
For any total recursive $g$ in the proof of Proposition~\ref{p:nonLearnablePhi}, also $f_g$ remains total recursive and therefore
has a G\"odel number $e$. The remaining arguments of both Proposition~\ref{p:nonLearnablePhi} and Proposition~\ref{p:gammaf} then remain valid,
except the fact that $e$ -- in contrast to $f_g$ (or even $\widehat{\theta_e}$) -- is not trivially majorizable.\\ 
That means it suffices to show the following. 
For any constant $D$ and any total recursive functions $L^*$, $B^*$, there is a total recursive function $g$, s.t. the following leads to a contradiction
\[ \forall x^0\ (L^*(e,x)\circ g)^{B^*(e,x)D}\geq g^{g^{x}(0)}(0),\]
where the G\"odel number $e$ is the G\"odel number of the total recursive function $f_g$ (note that we still have $\forall n^0 (\theta_e(n)\leq1)$).\\
We can argue similarly as in the proof of Proposition~\ref{p:nonLearnablePhi}, since for any fix $g$, eventually we have $x>e$, so we can simply choose
a total recursive $g$ which grows much faster than $L^*(x,x)$ and $B^*(x,x)$.
\end{proof}

\begin{cor}\label{c:gammaee}
Given a G\"odel numbering $\theta$ of total recursive functions, define the sequence
\[
\gamma_n := \max_{e\leq n} \big( \gamma(\theta_e)_n\big).
\]
The formula stating the existence of a Cauchy point of $\gamma\seq$
\[
\psi:=\forall x^0\exists z \forall k\geq z\ \big( \gamma_k<2^{-x} \big).
\]
is not effectively learnable.
\end{cor}
\begin{proof}
Suppose that $\rho^1$ is a Cauchy rate of $\gamma\seq$. It suffices to show that we can (effectively) derive a Cauchy rate $\rho_e$ for
any $\gamma(\theta_e)\seq$ from $\rho$.\\
Set \[\rho_e(x):=\max\big(e,\rho(x)\big),\]
and consider an arbitrary $e$ and $x$. Then
\[
k\geq\rho_e(x)\rightarrow k\geq\max\big(e,\rho(x)\big) \rightarrow \big(\gamma_k<2^{-x} \wedge k\geq e\big) 
\]
hence by definition of $\gamma\seq$
\[
\max_{e'\leq k} \big( \gamma(\theta_{e'})_k\big)<2^{-x} \wedge e\leq k
\]
so in particular $\gamma(\theta_{e})_k<2^{-x}$.
\end{proof}
% Prop on:
% Cauchy rate (in terms 
% of computable functionals $(B,L)$) for the sequence $\gamma_{(\cdot)}$. 
% ??

\section{When learnability implies fluctuation bounds}
%\begin{con}
%Suppose we analyze a proof of convergence of a sequence $x$ based on convergence of a monotone sequence $y$. 
%This means, assume we derive a rate of metastability $X(n_m,g)$ based on a rate of metastability $Y(m,f_g)$.
%Let us denote the computation of $n_m$ in $g$ by $A(m,g)$.\\
%Furthermore, suppose that we have $s$ fluctuations by at least $\epsilon$ indexed by $i_1,j_1$ up to $i_s,j_s$ and
%that there is an $h$ s.t.
%\[ 
%j_s>h(s)A(j_0,g).
%\]
%Then we can compute a bound on fluctuations of $x$ in $h$.
%
%\end{con}

In some cases fmc for the convergence rate (meaning that the convergence rate can be learned with finitely many mind changes) gives a bound on the number of fluctuations. This for instance is the case for bounded monotone sequences.
\\[2mm] 
We now consider the general form of the structure of Birkhoff's proof 
of the mean ergodic theorem as analyzed in \cite{Kohlenbach/Leustean4} and 
the argument used in \cite{Avigad/Rute} to convert the rate of metastability 
obtained in \cite{Kohlenbach/Leustean4} into a bound on the number of 
fluctuations: \\[1mm]
Let $x_{(\cdot)}$ be a sequence in some normed space $X$ and $y_{(\cdot)}$ be a sequence in $\RR_+$ definable by terms in $\ha[X,\|\cdot\|,\ldots].$ 
Suppose the Cauchyness of $x_{(\cdot)}$ 
is proved using that $y_{(\cdot)}$ has arbitrarily good approximate 
infima, i.e.
\begin{align}
\forall \delta>0\exists n &\forall k \forall \tilde k< k\ (y_{\tilde k}\geq y_n-\delta)\\
&\rightarrow \forall \epsilon>0\exists m \forall n \forall i,j\in[m;n]\ (\|x_i-x_j\|\leq \epsilon).
\end{align}
This implication is classically equivalent to 
\begin{align*}
\forall \epsilon>0 \exists \delta>0\ \Big( \exists n &\forall k \forall \tilde k< k\ (y_{\tilde k}\geq y_n-\delta)\rightarrow \\ \tag{+}\label{e:U2}
&\exists m \forall u \forall i,j\in[m;u]\ (\|x_i-x_j\|\leq \epsilon) \Big).
\end{align*}
Suppose now that 
\[
\ha[X,\|\cdot\|,\ldots]+\mbox{AC$+$M$^{\omega}+$IP}^{\omega}_{\forall} 
\vdash\text{\eqref{e:U2}}
\]
then
\begin{align*}
\ha&[X,\|\cdot\|,\ldots]+{\rm AC}+{\rm M}^{\omega} +{\rm IP}^\omega_\forall \vdash\\
&\forall\epsilon>0\exists\delta>0\forall n\exists m\geq n\forall u\exists k(\forall \tilde k<k(y_{\tilde k}\geq_\RR y_n-\delta)
\rightarrow \forall i,j\in[m;u](\|x_i-x_j\|<_\RR\epsilon).
\end{align*}
Hence by monotone functional interpretation one extracts terms 
 $\delta_\epsilon$, $m_\epsilon$ and $k_\epsilon$ (depending additionally only on majorants of the parameters $\underline{a}$ used in the definition of 
our sequences) s.t. (valid in 
${\cal S}^{\omega,X}$) for all majorants $\underline{a}^*$ of $\underline{a}$ 
\[
\forall \epsilon>0\ \forall n,u\ \bigg( m_\epsilon(n)>n \wedge \Big(\forall \tilde k< k_\epsilon(n,u)\ (y_{\tilde k}\geq y_n-\delta_\epsilon)\rightarrow 
 \forall i,j\in[m_\epsilon(n);u]\ (\|x_i-x_j\|\leq \epsilon) \Big) \bigg).
\tag{$*$}\label{e:U4-me}\]
Now define $k^*_\epsilon(u):=\max\{k_\epsilon(i,u)\ :\ i\leq u\}$ and consider
\[
\forall \epsilon>0\ \forall n,u\ \Big( \forall \tilde k< k^*_\epsilon(u)\ (y_{\tilde k}\geq y_n-\delta_\epsilon)\rightarrow 
 \forall i,j\in[m_\epsilon(n);u]\ (\|x_i-x_j\|\leq \epsilon) \Big).
\tag{$**$}\label{e:U4}\]
We can infer \eqref{e:U4} from \eqref{e:U4-me} by the following case 
distinction:\footnote{We are grateful to P. Oliva for pointing this out to us.}
\begin{itemize}
\item[Case]  1: $u<m_\epsilon(n)$. Then the conclusion and hence the whole implication is trivially true.
\item[Case]  2: $u\geq m_\epsilon(n) \geq n$.Then $k^*_\epsilon\geq k_\epsilon(n,u)$ and so $\forall \tilde k< k_\epsilon(u)\ (y_{\tilde k}\geq y_n-\delta_\epsilon)$ implies $\forall \tilde k< k_\epsilon(n,u)\ (y_{\tilde k}\geq y_n-\delta_\epsilon)$ and so the claim follows from $m_\epsilon(n) \geq n$.
\end{itemize}

Now suppose w.l.o.g. that $k_\epsilon:\NN\to\NN$ is injective and for any given $u$ define
\[
l_u\ :=\ k_\epsilon^{-1}(u)
\]
It follows directly from \eqref{e:U4} that the quantifier-free matrix holds also with $u:=l_u$, i.e.
\[
\forall \epsilon>0\ \forall n,u\ \Big( \forall \tilde k< u\ (y_{\tilde k}\geq y_n-\delta_\epsilon)\rightarrow 
 \forall i,j\in[m_\epsilon(n);k_\epsilon^{-1}(u)]\ (\|x_i-x_j\|\leq \epsilon) \Big).
\tag{-}\label{e:U5}\]
Now let $N_0$, $N_1$, ..., $N_{S_\epsilon}$ be integers s.t. $N_0=1$ and $N_{i+1}$ is the least $m>N_i$ s.t. $y_m<y_{N_i}-\delta_\epsilon$ as long as such an $m$ exists.
Assume that $b\geq y_1$ (for some $b$) and so $S_\epsilon\leq\frac{b}{\delta_\epsilon}$.\\
By \eqref{e:U5} there are no $\epsilon$-fluctuations of $x_{(\cdot)}$ on the $S$ many intervals $[m_\epsilon(N_i),k^{-1}_\epsilon(N_{i+1})]$ for $i=0,\ldots,S-1$.\\
In the intervals $[k^{-1}_\epsilon(N_{i}),m_\epsilon(N_i)]$ for $i=1,\ldots,S-1$ and $[0,m_\epsilon(N_0)]$ we have to show that if we have for any $N\in\NN$ $s$ many
fluctuations indexed within $[k^{-1}_\epsilon(N),m_\epsilon(N)]$ (or in $[0,m_\epsilon(N_0)]$) each indexed by a pair of indexes $(i,j)$ then the highest index of such fluctuation ($j_s$) has to be greater than (or equal to) some $\phi_\epsilon(s,N)$, where $\phi_\epsilon$ is such that
\[
\exists \tilde s\forall n\ \big(\phi_\epsilon(\tilde s,n)>m_\epsilon(n)\big).
\]
Then, given such an $\tilde s$, we have at most
\[
\frac{b}{\delta_\epsilon}+\tilde s\left(\frac{b}{\delta_\epsilon}+1\right)
\]
many fluctuations.


\begin{rmk}
Note that it might be even possible to compute $\tilde s$ in terms of $\phi_\epsilon$ and $m_\epsilon$. E.g. in Avigad we have:
\begin{align*}
\delta_\epsilon &:= \frac{\epsilon^2\mu}{16b},& m_\epsilon(n)&:=\big\lceil \frac{16b}{\epsilon}\big\rceil n,\\
k^{-1}_\epsilon(n)&:=\frac{n}{2},& \phi_\epsilon(s,n)&:=\big(1+ \frac{\epsilon}{2b}\big)^s n,
\end{align*}
and so \[ \tilde s \leq \frac{ 2 b\log \big\lceil \frac{16b}{\epsilon}\big\rceil }{\epsilon}.\]
For CAT$(0)$:
\begin{align*}
\delta_\epsilon &:= \frac{\epsilon^2}{14(M+1)^2},& m_\epsilon(n)&\approx \Theta_n(\frac{\epsilon^2}{4})n^6\approx n^6,\\
k^{-1}_\epsilon(n)&:=\frac{3nM^2}{\epsilon}.& 
\end{align*}

\end{rmk}

\begin{prop}
If there is a bound $B$ on the number of fluctuations at all, then there is an in $B$ primitive recursive $\phi_\epsilon$
satisfying the conditions in the proof:
\begin{align}
\forall s,N,i,j \big( i,j\tilde\in[k^{-1}_\epsilon(N),m_\epsilon(N)]\ \wedge\ \Fluc(s,i,j)\ \rightarrow j_s\geq \phi_\epsilon(s,N) \big),\label{e:FU1}\\
\exists \tilde s\forall n\ \big(\phi_\epsilon(\tilde s,n)>m_\epsilon(n)\big).\label{e:FU2}
\end{align}
\end{prop}
\begin{proof}
Set
\[
\phi_\epsilon(s,n):=\begin{cases}
k^{-1}_\epsilon(n)&\Tif\ s\leq B,\\
m_\epsilon(n)&\Telse.
\end{cases}
\]
\end{proof}

\begin{rmk}
In particular, this means that if we know there is no computable $\phi$ (in parameters $\tup a$) satisfying these conditions,
then there cannot be a bound on the number of fluctuations, which is computable (in parameters $\tup a$).
\end{rmk}

\subsection*{Acknowledgements}
This research was supported by the German Science Foundation (DFG Project KO 1737/5-1). 


\begin{thebibliography}{00}
\bibitem{Akama} Akama, Y., Berardi, S., Hayashi, S., Kohlenbach, U., 
An arithmetical hierarchy of the law of excluded middle and related 
principles. Proc. of the 19th Annual IEEE Symposium on Logic in 
Computer Science (LICS'04), pp. 192-201, IEEE Press (2004).
\bibitem{Aschieri1} Aschieri, F., A contructive analysis of learning 
in Peano Arithmetic. To appear in: Ann. Pure Appl. Logic.
\bibitem{Aschieri2} Aschieri, F., Learning based on realizability 
for HA+EM1 and 1-Backtracking games: Soundness and completeness. 
To appear in: Ann. Pure Appl. Logic.
\bibitem{Aschieri/Berardi} Aschieri, F., Berardi, S., 
A new use of Friedman's translation: interactive realizability.  
in: Logic, Construction, Computation, Ontos-Verlag Series in 
Mathematical 
Logic, Berger et al. editors, 2012
\bibitem{Avigad/Gerhardy/Towsner} Avigad, J., Gerhardy, P., Towsner, H., 
Local stability of ergodic averages. Trans. Amer. 
Math. Soc. {\bf 362}, pp. 261-288 (2010). 
\bibitem{Avigad/Rute} Avigad, J., Rute, J., Oscillation and the 
mean ergodic theorem. Preprint 2012.
\bibitem{Baillon(75)} Baillon, J.B., Un th\'eor\`eme de type ergodique 
pour les contractions non lin\'eaires dans un espace de Hilbert. 
C.R. Acad. 
Sci. Paris S\`er. A-B {\bf 280}, pp. 1511-1514 (1975).
\bibitem{Baillon(76)} Baillon, J.B., Quelques propri\'et\'es de 
convergence asymptotique pour les contractions impaires. 
C.R. Acad. 
Sci. Paris S\`er. A-B {\bf 283}, pp. 587-590 (1976). 
\bibitem{Berardi/Coquand/Hayashi} Berardi, S., Coquand, T, Hayashi, S., 
Games with 1-Backtracking. Ann. Pure Appl. Logic {\bf 161}, pp. 
1254-1264 (2010).
\bibitem{Coquand} Coquand, T., A semantics of evidence for classical
arithmetic. J. Symbolic Logic {\bf 60}, pp. 325-337 (1995).
\bibitem{GerKoh06}
Gerhardy, P., Kohlenbach, U.,  
Strongly uniform bounds from semi-constructive proofs, Ann. Pure Appl. 
Logic {\bf 141}, pp. 89-107 (2006).
\bibitem{Hayashi/Nakata} Hayashi, S., Nakata, M., Towards limit computable 
mathematics. In: P. Callaghan et al. (eds.), TYPES 2000, Springer 
LNCS {\bf 2277}, pp. 125-144 (2002).
\bibitem{Jones} Jones, R.L., Ostrovskii, I.V., Rosenblatt, J.M., 
Square functions in ergodic theory. Ergodic Theory and Dynamical 
Systems {\bf 16}, pp. 267-305 (1996).
\bibitem{Kohlenbach(lowrate)} Kohlenbach, U., Mathematically strong subystems of analysis with low rate of growth of provably recursive functionals. Arch. Math. Logic {\bf 36}, pp. 31-71 (1996). 
\bibitem{Kohlenbach(relative)} Kohlenbach, U., Relative constructivity.
J. Symbolic Logic {\bf 63}, pp. 1218-1238 (1998). 
\bibitem{Kohlenbach(metapaper)} Kohlenbach, U., Some logical
metatheorems with applications in functional analysis. 
Trans. Amer. Math. Soc. {\bf 357}, no. 1, pp. 89-128 (2005).
\bibitem{Kohlenbach(book)} Kohlenbach, U., Applied Proof Theory: 
Proof Interpretations and their Use in Mathematics. 
Springer Monographs in Mathematics. xx+536pp., Springer 
Heidelberg-Berlin, 2008.
\bibitem{Kohlenbach(Browder)} Kohlenbach, U.,On quantitative 
versions of  theorems due to F.E. Browder and R. Wittmann. Advances in 
Mathematics {\bf 226}, pp. 2764-2795 (2011).   
\bibitem{Kohlenbach(Baillon)} Kohlenbach, U., A uniform quantitative form of 
sequential weak compactness and Baillon's nonlinear ergodic theorem. 
Communications in Contemporary Mathematics {\bf 14}, 20pp. (2012). 
\bibitem{Kohlenbach/Leustean3} Kohlenbach, U., Leu\c{}stean, L., 
Asymptotically nonexpansive mappings in uniformly convex hyperbolic spaces. 
Journal of the European Mathematical Society {\bf 12}, pp. 71-92 (2010). 
\bibitem{Kohlenbach/Leustean4} Kohlenbach, U., Leu\c{s}tean, L., A 
quantitative mean ergodic theorem for uniformly convex Banach spaces. 
Ergodic Theory and Dynamical Systems {\bf 29}, pp. 1907-1915 (2009). 
\bibitem{Kohlenbach/Leustean6} Kohlenbach, U., Leu\c{s}tean, L., 
Effective metastability of Halpern iterates in CAT(0) spaces.
Adv. Math. {\bf 231}, pp. 2526-2556 (2012).
\bibitem{Kohlenbach/Leustean7} Kohlenbach, U., Leu\c{s}tean, L., 
On the computational content of convergence proofs via Banach limits. 
Philosophical Transactions of the Royal Society A  {\bf 370}, 
pp. 3449-3463 (2012). 
\bibitem{Luckhardt(89)} Luckhardt, H., Herbrand-Analysen zweier Beweise
des Satzes von Roth: Polynomiale Anzahlschranken. J. Symbolic Logic
{\bf 54}, pp. 234-263 (1989).
\bibitem{Saejung} Saejung, S., Halpern's iteration in CAT(0) spaces. 
Fixed Point Theory and Applications {\bf 2010} (2010).
\bibitem{Safarik(11)} Safarik, P., A quantitative nonlinear strong 
ergodic theorem for Hilbert spaces. J. Math. Anal. Appl. {\bf 391}, 
pp. 26-37 (2012).
\bibitem{Sieg} Sieg, W., Fragments of arithmetic. Ann. Pure Appl. Logi. 
{\bf 28}, pp. 33-71 (1985).
\bibitem{Toftdal} Toftdal, M., Calibration of Ineffective Theorems of 
Analysis in a Constructive Context. Master Thesis, Aarhus Universitet, 
2004.
\bibitem{Wittmann(90)} Wittmann, R., Mean ergodic theorems for nonlinear 
operators. Proc. Amer. Math. Soc. {\bf 108}, pp. 781-788 (1990).
\bibitem{Wittmann(92)} Wittmann, R., Approximation of fixed points of 
nonexpansive mappings. Arch. Math. {\bf 58}, pp. 486-491 (1992).
\end{thebibliography}

\end{document}